@article{__1999,
  title = {农作物长势的定义与遥感监测},
  author = {{杨邦杰裴志远}},
  year = {1999},
  journal = {农业工程学报},
  number = {03},
  pages = {214--218},
  abstract = {监测作物生长过程的状况与趋势，即长势监测是农业遥感更为重要的任务。其目的是：１）为田间管理提供及时的信息；２）早期估计产量。该文以冬小麦为例，根据实地调查与北方数省的资料，用作物的个体与群体特征定义作物长势， 讨论了遥感监测的可能性，提出了基于植被指数与植被表面温度的长势遥感监测的评估模型与诊断模型的概念与算法。},
  keywords = {NDVI,农作物长势,表面温度,遥感监测},
  file = {C\:\\Users\\HP\\Zotero\\storage\\WAE8EFPG\\农作物长势的定义与遥感监测.pdf}
}

@article{__1999-1,
  title = {农作物长势的定义与遥感监测},
  author = {杨邦杰 and 裴志远},
  year = {1999},
  journal = {农业工程学报},
  number = {03},
  pages = {214--218},
  abstract = {监测作物生长过程的状况与趋势，即长势监测是农业遥感更为重要的任务。其目的是：１）为田间管理提供及时的信息；２）早期估计产量。该文以冬小麦为例，根据实地调查与北方数省的资料，用作物的个体与群体特征定义作物长势， 讨论了遥感监测的可能性，提出了基于植被指数与植被表面温度的长势遥感监测的评估模型与诊断模型的概念与算法。},
  isbn = {1002-6819},
  lccn = {11-2047/S},
  keywords = {农作物长势;遥感监测;NDVI;表面温度}
}

@article{__2014,
  title = {基于 {{CERES-Maize}} 模型的吉林西部玉米干旱脆弱性曲线研究},
  author = {{董姝娜} and {庞泽源} and {张继权} and {佟志军} and {刘兴朋} and {孙仲益}},
  year = {2014},
  journal = {灾害学},
  volume = {29},
  number = {3},
  pages = {115--119}
}

@article{__2016,
  title = {农业遥感研究应用进展与展望},
  author = {陈仲新 and 任建强 and 唐华俊 and 史云 and 冷佩 and 刘佳 and 王利民 and 吴文斌 and 姚艳敏 and 哈斯图亚},
  year = {2016},
  journal = {遥感学报},
  volume = {20},
  number = {05},
  pages = {748--767},
  abstract = {得益于中国自主遥感卫星、无人机遥感和物联网等技术的发展,中国农业遥感研究与应用在过去20年取得了显著进步,中国农业遥感信息获取呈现出天地网一体化的趋势;农业定量遥感在关键参数遥感反演技术方法与应用方面取得进展;作物面积、长势、产量、灾害遥感监测的理论与技术方法取得突破,农业遥感技术应用领域不断拓展。本文从农业遥感信息获取、农业定量遥感、农业灾害遥感、作物遥感识别与制图、作物长势遥感监测与产量预测、农业土地资源遥感等方面对中国农业遥感科研与应用进行了总结综述。},
  isbn = {1007-4619},
  lccn = {11-3841/TP},
  keywords = {农业;遥感;进展;展望},
  file = {C\:\\Users\\HP\\Zotero\\storage\\CJEYT3DR\\陈仲新 等 - 2016 - 农业遥感研究应用进展与展望.pdf}
}

@article{__2018,
  title = {作物模型在我国玉米生产中的研究与应用},
  author = {{陈小民} and {崔红} and {刘志铭} and {魏雯雯} and {姚凡云} and {吴杨} and {曹玉军} and {吕艳杰} and {王永军} and {Hong Cui} and {Zhiming Liu} and {Wenwen Wei} and {Fanyun Yao} and {Yang Wu} and {Yujun Cao} and {Yanjie Lu} and {Yongjun Wang}},
  year = {2018},
  journal = {玉米科学},
  volume = {26},
  number = {03},
  pages = {115--120},
  abstract = {作物模型在我国发展与应用日趋完善,已成为农业科研领域重要的研究工具,在区域化模拟、农业预测与风险分析、宏观农业决策制定、未来气候情景模拟、优化栽培措施及气象灾害影响评估研究中发挥重要作用,极大地推动了我国农业生产发展和科研水平的提高。对当前国内作物模型研究中具有代表性的6个生长模型概况和在玉米上应用进展进行综述,为研究者根据研究目的及具体条件选择适宜的作物模型进行应用提供参考,进一步提高作物模型在我国玉米科研与生产中的应用水平。对现有主要模型存在问题及今后需开展的工作进行阐述。},
  keywords = {作物模型,发展趋势,玉米,研究应用},
  file = {C\:\\Users\\HP\\Zotero\\storage\\H5ITJNQD\\作物模型在我国玉米生产中的研究与应用_陈小民.pdf}
}

@article{__2019,
  title = {基于深度学习的农业植物表型研究综述},
  author = {{基于深度学习的农业植物表型研究综述}},
  year = {2019},
  journal = {SCIENTIA SINICA Vitae},
  volume = {49},
  number = {6},
  pages = {698--716},
  issn = {1674-7232},
  doi = {10.1360/SSV-2019-0020}
}

@article{__2019-1,
  title = {深度学习在大田种植中的应用及展望},
  author = {{郭祥云} and {台海江} and {Haijiang Tai}},
  year = {2019},
  journal = {中国农业大学学报},
  volume = {24},
  number = {01},
  pages = {119--129},
  abstract = {深度学习是目前机器学习领域最前沿和最具前景的技术,本研究采用归纳总结法,介绍了深度学习的特征及与传统机器学习的区别,归纳和梳理了深度学习在大田种植中的应用现状。结果表明:1)深度学习在大田种植中的应用初现端倪,主要集中在作物的识别与分类、农业遥感影像应用、土壤环境监测、农业场景识别等;2)采用的主要模型有卷积神经网络(CNN)、自编码(AE)、深度置信网络(DBN)、堆栈自编码(SAE)、全卷积神经网络(FCN)、深度神经网络(DCNN)等,其对各领域的分类与识别精度均有提高;3)目前存在的主要问题是标注数据缺乏,尤其在遥感图像分类领域,普遍采用了迁移学习、数据增强、微调等技术来解决标注数据缺乏的问题。随着大田种植领域数据的增长以及信息技术的快速发展,基于深度学习和多源异构数据的作物识别与分类、作物长势监测、病虫害预测预警、农作物产量预测、果树花朵及果体识别、水果质量及产量的优化控制等将会获得较快发展。},
  keywords = {大数据,大田种植,智慧农业,机器学习,深度学习},
  file = {C\:\\Users\\HP\\Zotero\\storage\\TVNFDEE6\\深度学习在大田种植中的应用及展望.pdf}
}

@article{__2019-2,
  title = {深度学习在我国农业中的应用研究现状},
  author = {{吕盛坪} and {李灯辉} and {冼荣亨} and {Denghui Li} and {Rongheng Xian}},
  year = {2019},
  journal = {计算机工程与应用},
  volume = {55},
  number = {20},
  pages = {24-33+51},
  abstract = {深度学习(Deep Learning,DL)已广泛应用于智能农业的病虫害检测、植物和水果识别、农作物及杂草检测与分类等研究中。对2014年至2019年国内发表的65篇有关DL在农业中应用研究成果进行综述。简要介绍DL的基本概念及其发展历史,给出了所选论文检索方法及其分布;对所选论文从研究对象与目的、数据来源、类间差异、预处理、数据扩增、模型框架以及性能对比等角度进行了综述;对DL的优缺点进行了分析,并指明了其在智能农业研究中的发展趋势。},
  keywords = {分类,智能农业,检测,深度学习,识别,预测},
  file = {C\:\\Users\\HP\\Zotero\\storage\\S8TQ53D2\\深度学习在我国农业中的应用研究现状_吕盛坪.pdf}
}

@article{__2019-3,
  title = {作物长势评估指数的设计与应用},
  author = {{张蕾} and {侯英雨} and {郑昌玲} and {刘维} and {何亮} and {郭安红} and {程路}},
  year = {2019},
  journal = {应用气象学报},
  volume = {30},
  number = {5},
  pages = {543--554}
}

@article{__2019-4,
  title = {深度学习在我国农业中的应用研究现状},
  author = {吕盛坪 and 李灯辉 and 冼荣亨},
  year = {2019},
  journal = {计算机工程与应用},
  volume = {55},
  number = {20},
  pages = {24-33+51},
  abstract = {深度学习(Deep Learning,DL)已广泛应用于智能农业的病虫害检测、植物和水果识别、农作物及杂草检测与分类等研究中。对2014年至2019年国内发表的65篇有关DL在农业中应用研究成果进行综述。简要介绍DL的基本概念及其发展历史,给出了所选论文检索方法及其分布;对所选论文从研究对象与目的、数据来源、类间差异、预处理、数据扩增、模型框架以及性能对比等角度进行了综述;对DL的优缺点进行了分析,并指明了其在智能农业研究中的发展趋势。},
  isbn = {1002-8331},
  keywords = {深度学习;智能农业;检测;识别;分类;预测}
}

@article{__2019-5,
  title = {基于注意力卷积模块的深度神经网络图像识别},
  author = {{袁嘉杰} and {张灵} and {陈云华}},
  year = {2019},
  journal = {计算机工程与应用},
  volume = {55},
  number = {8},
  pages = {9--16}
}

@article{__2019-6,
  title = {作物长势评估指数的设计与应用},
  author = {{张蕾} and {侯英雨} and {郑昌玲} and {刘维} and {何亮} and {郭安红} and {程路}},
  year = {2019},
  journal = {应用气象学报},
  volume = {30},
  number = {5},
  pages = {543--554}
}

@article{__2019-7,
  title = {作物长势评估指数的设计与应用},
  author = {{张蕾} and {侯英雨} and {郑昌玲} and {刘维} and {何亮} and {郭安红} and {程路}},
  year = {2019},
  journal = {应用气象学报},
  volume = {30},
  number = {5},
  pages = {543--554}
}

@phdthesis{__2021,
  title = {气候变化对棉花生长和产量的影响},
  author = {{李娜}},
  year = {2021},
  abstract = {棉花是世界上种植最广泛的纤维作物,在国民经济中有重要作用。气候变化通过温度与降水的变化,使日照、水分、土壤等作物生长的环境要素发生改变,进而对棉花的物候期、生长潜力和种植制度等产生影响,最终影响产量。研究棉花生长对气候变化的反馈机制,对棉花种植规划和增产稳产具有重要实际意义。本研究在收集棉花生长、管理措施、气象和土壤等数据的基础上,首先利用线性倾向率和MMK趋势检验方法分析了棉花生长期内的主要气象要素的时空变化特征;其次基于一阶差分法、Pearson相关系数、通径分析和多元线性回归方法分析了棉花各物候期的变化趋势,分离了作物管理和气候变化对棉花各物候期的影响,探究了棉花物候和产量变化之间的关系;然后基于统计分析方法和AquaCrop作物模型探究了气候变化和棉花生长、产量之间的定性和定量关系;最后利用Meta分析方法,汇编了已发表的相关研究成果,探究了棉花产量对温度、降水、CO\_2浓度变化和适用措施的响应。主要得出了以下结论:(1)利用一阶差分法、Pearson相关系数、通径分析和多元线性回归方法分析了位于我国棉花主产区79个站点气候变化对棉花各物候期的影响。研究发现棉花的出苗、现蕾、开花和吐絮日期分别提前了0.026～0.351天/年。播种期和成熟期分别推迟0.170和0.337天/年。播种-出苗,现蕾-开花,和开花-吐絮期平均缩短了0.19～0.30天/年,出苗-现蕾、裂铃-成熟和整个生长期(播种-成熟)分别延长了0.11、0.77和0.082天/年。除裂铃-成熟期外,降水会缩短大部分物候期的长度。日照对棉花各物候期的影响与降水恰好相反。一般来说,平均温度、最低温和最高温的增加缩短了棉花各物候期长度。气候变化对棉花各物候期的影响小于作物管理的单独影响和气候变化与作物管理的综合影响。在综合影响和作物管理的单独影响下,棉花播种-出苗和现蕾-开花期被缩短,其他物候期被延长。气候变化的单独影响缩短了播种-出苗、出苗-现蕾和开花-裂铃期的长度、延长了现蕾-开花、裂铃-成熟和播种-成熟期,这意味着气候变化下持续时间较长的棉花品种可能是更好的种植选择,这可能是适应气候变化的可行策略。此外,研究发现,播种、出苗、现蕾、开花和裂铃日期的推迟会降低籽棉产量,但是随着棉花播种-出苗、开花-裂铃、裂铃-成熟和播种-成熟期长度的增加,籽棉产量增加。(2)基于实际观测的长时间序列数据,探究了位于新疆地区的19个站点的气候变化对棉花生长和产量的影响。研究结果表明棉花开花期株高,籽棉产量,棉秆重和衣分率在大多数站点显示出显著增加趋势。棉花生长期内的温度变化(最高温、最低温和平均温度)对籽棉产量和棉杆重有正面影响,但对棉花开花期株高和衣分率有负面影响。此外,开花期株高、籽棉产量和衣分率对降水的变化具有积极反应,但在不同站点中对日照时数和平均空气相对湿度变化的响应表现出不确定性,具有一定的区域特征。Pearson相关系数被用于分析每种棉花生长指标与气象要素之间的相关性,研究结果表明相关性有正有负,并且在大多数站点中是不显著的。此外,通过逐步线性回归方法建立的模型比Pearson的单变量相关性更好,并且在每个站点中模型精度都得到了改善。引入非线性变量后选定的最佳回归模型,可以分别解释棉花开花期株高、籽棉产量、棉秆重和衣分率的8.1～69.9\%、8.5～75.3\%、7.1～56.1\%和10.4～85.7\%的的变化。(3)基于AquaCrop模型分析了位于我国棉花主产区的40个站点的气候变化对棉花生长的影响。研究结果表明AquaCrop模型对我国棉花具有较好的适用性,可以较好地模拟棉花的生长过程。1978～2018年研究区域大部分站点的降水呈现不显著增加趋势,平均温度、最低温度、最高温度和积温呈显著增加趋势,平均空气湿度、风速和昼夜温差主要呈现减少趋势,而日照时数和太阳辐射在西北内陆区表现为增加趋势,在黄河和长江流域棉区主要为减少趋势。模拟的棉花地上部生物量、潜在产量和水分利用效率在98\%、98\%和93\%的站点中呈现增加趋势,且在大部分站点中呈显著增加趋势,而棉花生长期间的实际蒸发蒸腾量主要呈现减少趋势。CO\_2浓度增加是造成棉花产量增加的主要原因。通过计算棉花生长指标和气象要素之间的皮尔逊相关系数和逐步回归方程发现,影响棉花地上部生物量的主要气象要素是风速、太阳辐射和最高温度;影响实际蒸发蒸腾量的主要气象要素是风速、太阳辐射、日照时数和平均空气相对湿度;影响潜在产量的主要气象要素是风速、太阳辐射、日照时数和最低温。棉花生育期内的气候变量解释了实际蒸发蒸腾量、地上部生物量、潜在产量和水分利用效率65.1～95.5\%、9.7～74.5\%、14.8～68.3\%、15.2～90.4\%的变化。总的来说,棉花的生长与风速、太阳辐射、最低和最高温度的变化密切相关。(4)基于AquaCrop模型模拟了未来气候变化对我国棉花产量的影响及适应措施。研究发现SSP2-4.5和SSP5-8.5排放情景下的气候预测数据显示,位于我国棉花主产区的40个站点的温度呈现显著增加趋势,尤其是在在高排放情景下(SSP5-8.5)。棉花生育期内的降水量也主要呈现增加趋势,但时空变异性较大,且SSP5-8.5情景2080s的降水量变化最大。棉花生长期内的参考作物蒸发蒸腾量在SSP2-4.5情景下的2040s主要表现减少趋势,而在SSP2-4.5情景下的2080s和SSP5-8.5情景下主要呈现增加趋势。未来气候变化对灌溉棉花产量有正向促进作用。与基准期相比,在SSP2-4.5情景下,棉花产量在2040s和2080s分别平均增加10.51\%和13.67\%;在SSP5-8.5情景下,棉花产量在2040s和2080s分别增加14.76\%和28.04\%,SSP5-8.5情景下的棉花产量增加比SSP2-4.5情景下更为显著,这是由于SSP5-8.5情景下未来CO\_2浓度远大于SSP2-4.5,CO\_2的肥效作用更显著导致的。此外,早播有利于棉花增产,与常规播种日期(4月20日)相比,早播(3月31日)可以使大部分站点2040s和2080s两个时期的棉花产量在SSP2-4.5情景下平均提高1.99\%和2.41\%,在SSP5-8.5情景下分别提高1.88\%和2.63\%。这一发现支持我国棉花生产采用早播种植策略,以在未来气候条件下优化产量。(5)通过汇编所有已发表的相关研究结果,利用Meta分析方法定性和定量探究了气候变化对棉花产量的可能影响及不确定性。研究发现温度、降水量、CO\_2浓度和适应措施对棉花产量有显著影响。气温每升高1\textcelsius (0.26-6.08\textcelsius ),棉花产量下降7.79\%;降水量和CO\_2浓度每增加1 mm和1 ppm,棉花产量分别增加0.102\%和0.05\%。在一定的适应性措施(如改变播期、品种、灌水量等)下,棉花产量比未采取适应性措施时提高32.65\%。升高的CO\_2浓度对棉花产量的作用也是指示气候变化影响的信号和程度的关键。考虑到这一影响,未来产量形势将更加乐观。适应措施(改变播种日期和品种)也可以在一定程度上缓解气候变化的负面影响。并且通过比较单个或多个GCM({$\geq$}2)模式模拟的棉花产量变化百分比可以发现气候模式数量的增加可以降低棉花产量变化的中值范围和异常值。因此,增加GCM的数量减少了气候变化影响研究中作物模型模拟的不确定性。此外,研究区域、作物模型和气候情景的差异对棉花产量变化的影响显著,这主要取决于当地气候条件、土壤性质、耕作方式和管理实践。},
  school = {西北农林科技大学, M1  - 博士},
  keywords = {AquaCrop模型,时空变化,棉花,气象要素,统计分析}
}

@article{_apsim_2016,
  title = {{{基于APSIM模型的气候变化对西南春玉米产量影响研究}}},
  author = {{戴彤} and {王靖} and {赫迪} and {王娜} and {Jing Wang} and {Di He} and {Na Wang}},
  year = {2016},
  journal = {资源科学},
  volume = {38},
  number = {01},
  pages = {155--165},
  abstract = {为研究西南地区春玉米生长季主要气象因子对产量的影响,本文利用农业气象试验站作物及土壤资料,评价了APSIM-Maize模型在中国西南地区的适应性,并应用其分析该地区1961-2010年春玉米雨养产量的时空变化特征,明确了春玉米雨养产量的影响因子及影响程度。研究结果表明:APSIM模型对该区6个常用玉米品种的模拟效果较好,模拟与实测生育期的均方根误差(RMSE)在8d以内;4个品种地上部分生物量以及产量的模拟值与实测值归一化均方根误差(NRMSE)均低于29\%,该模型在西南地区具有较好的适应性。研究区域春玉米生长季总辐射在南部中区和北部降低最明显,{$\geq$}8\textcelsius 有效积温在西部升高显著,日均温度日较差在西部和东南部减小最显著,总降水在区域中部减少较显著。模拟的春玉米雨养产量在全区46\%的研究站点中呈显著降低趋势(P{$<$}0.05),尤其东部中区和南部最显著;减产显著的站点中,生长季辐射降低、温度升高、降水减少和温度日较差降低对减产的贡献率分别为32\%、40\%、1\%和-2\%。},
  keywords = {春玉米,模型,西南地区,调参验证,贡献率,逐步回归,雨养产量}
}

@article{_cgmps_nodate,
  title = {{{CGMPS}}: {{An Innovative Crops Growth Monitoring}} and {{Prediction System}}},
  author = {晟喆},
  file = {C\:\\Users\\HP\\Zotero\\storage\\ZMXMTXQ4\\Crops_Growth_Monitoring_and_Prediction_System.pdf}
}

@article{adisa_application_2019,
  title = {Application of Artificial Neural Network for Predicting Maize Production in {{South Africa}}},
  author = {Adisa, Omolola M and Botai, Joel O and Adeola, Abiodun M and Hassen, Abubeker and Botai, Christina M and Darkey, Daniel and Tesfamariam, Eyob},
  year = {2019},
  journal = {Sustainability},
  volume = {11},
  number = {4},
  pages = {1145},
  publisher = {{MDPI}}
}

@article{aggarwal_uncertainties_1995,
  title = {Uncertainties in Crop, Soil and Weather Inputs Used in Growth Models: {{Implications}} for Simulated Outputs and Their Applications},
  shorttitle = {Uncertainties in Crop, Soil and Weather Inputs Used in Growth Models},
  author = {Aggarwal, P.K.},
  year = {1995},
  month = jan,
  journal = {Agricultural Systems},
  volume = {48},
  number = {3},
  pages = {361--384},
  issn = {0308521X},
  doi = {10.1016/0308-521X(94)00018-M},
  abstract = {Deterministic crop growth models require several inputs relating to crop/ variety, soil physical properties, weather and crop management. The input values used could be significantly uncertain due to random and systematic measurement errors and spatial and temporal variation observed in many of these inputs. Often soil and weather data are approximated using GIS and/ or weather generators. In this paper total uncertainty in simulated yield, evapotranspiration and crop N uptake has been quantified considering uncertainties in crop, soil and weather inputs. WTGRO WS, a crop model that simulates the e\#ect of genotypic, climatic, edaphic and management factors on productivity of spring wheat was used. The uncertainty in each input was represented by a statistical distribution of values based on literature review, actual measurement and subjective expert judgement. The Monte Carlo simulation technique was used to analyze total uncertainty.},
  langid = {english},
  file = {C\:\\Users\\HP\\Zotero\\storage\\NEZ2RD2N\\Aggarwal - 1995 - Uncertainties in crop, soil and weather inputs use.pdf}
}

@article{azzari_towards_2017,
  title = {Towards Fine Resolution Global Maps of Crop Yields: {{Testing}} Multiple Methods and Satellites in Three Countries},
  author = {Azzari, George and Jain, Meha and Lobell, David B},
  year = {2017},
  journal = {Remote Sensing of Environment},
  volume = {202},
  pages = {129--141},
  publisher = {{Elsevier}}
}

@article{baltrusaitis_multimodal_2018,
  title = {Multimodal Machine Learning: {{A}} Survey and Taxonomy},
  author = {Baltru{\v s}aitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
  year = {2018},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {41},
  number = {2},
  pages = {423--443},
  publisher = {{IEEE}}
}

@inproceedings{brasoveanu_visualizing_2020,
  title = {Visualizing Transformers for Nlp: A Brief Survey},
  booktitle = {2020 24th {{International Conference Information Visualisation}} ({{IV}})},
  author = {Bra{\c s}oveanu, Adrian MP and Andonie, R{\u a}zvan},
  year = {2020},
  pages = {270--279},
  publisher = {{IEEE}}
}

@article{cai_high-performance_2018,
  title = {A High-Performance and in-Season Classification System of Field-Level Crop Types Using Time-Series {{Landsat}} Data and a Machine Learning Approach},
  author = {Cai, Yaping and Guan, Kaiyu and Peng, Jian and Wang, Shaowen and Seifert, Christopher and Wardlow, Brian and Li, Zhan},
  year = {2018},
  journal = {Remote sensing of environment},
  volume = {210},
  pages = {35--47},
  publisher = {{Elsevier}}
}

@article{campbell_effect_1988,
  title = {Effect of Crop Rotation and Fertilization on the Quantitative Relationship between Spring Wheat Yield and Moisture Use in Southwestern {{Saskatchewan}}},
  author = {Campbell, CA and Zentner, RP and Johnson, PJ},
  year = {1988},
  journal = {Canadian Journal of Soil Science},
  volume = {68},
  number = {1},
  pages = {1--16},
  publisher = {{NRC Research Press Ottawa, Canada}}
}

@article{carlson_relation_1997,
  title = {On the Relation between {{NDVI}}, Fractional Vegetation Cover, and Leaf Area Index},
  author = {Carlson, Toby N and Ripley, David A},
  year = {1997},
  journal = {Remote sensing of Environment},
  volume = {62},
  number = {3},
  pages = {241--252},
  publisher = {{Elsevier}}
}

@article{chen_heu_2021,
  title = {{{HEU Emotion}}: A Large-Scale Database for Multimodal Emotion Recognition in the Wild},
  author = {Chen, Jing and Wang, Chenhui and Wang, Kejun and Yin, Chaoqun and Zhao, Cong and Xu, Tao and Zhang, Xinyi and Huang, Ziqiang and Liu, Meichen and Yang, Tao},
  year = {2021},
  journal = {Neural Computing and Applications},
  volume = {33},
  number = {14},
  pages = {8669--8685},
  publisher = {{Springer}}
}

@article{chen_improving_2018,
  title = {Improving Regional Winter Wheat Yield Estimation through Assimilation of Phenology and Leaf Area Index from Remote Sensing Data},
  author = {Chen, Yi and Zhang, Zhao and Tao, Fulu},
  year = {2018},
  journal = {European Journal of Agronomy},
  volume = {101},
  pages = {163--173},
  issn = {1161-0301},
  doi = {10.1016/j.eja.2018.09.006},
  abstract = {Crop yield estimation at regional scale using crop model is generally subjected to large uncertainties from insufficient spatial information on heterogeneous growth environment and agronomic management practices. To solve this problem, we assimilated crop phenology and leaf area index (LAI) derived from remote sensing into a crop model (MCWLA-Wheat) to improve its reliability in estimating winter wheat yields at regional scale. Since the LAI magnitude was obviously underestimated however its spatial pattern was relatively well captured by remote sensing, we developed a novel spatial assimilation scheme that assimilated the spatial differences instead of the absolute values of LAI into crop model. Firstly, we retrieved the information of critical development stages of winter wheat from remote sensing data to adjust the simulation of phenology by MCWLA-Wheat model; then the spatial differences of LAI derived from remote sensing were assimilated into the MCWLA-Wheat model using a kind of constant gain Kalman Filter algorithm to improve the ability of the model in estimating winter wheat LAI and yields at regional scale in the North China Plain. This assimilation scheme extracted effective information from remote sensing LAI and meanwhile abandoned the information with obvious errors, ensuring that the assimilation variables could be close to the reality. It avoids the requirement for correction of the LAI derived from remote sensing using other high-quality ancillary data from field measurements. Using this assimilation scheme, the performance of crop model improved substantially. It successfully produced more accurate yield estimates at regional scale during the period of 2001\textendash 2008 (mean R2 = 0.42, RMSE = 737/ha) than those without assimilation (mean R2 = 0.26, RMSE = 1012 kg/ha) and those directly assimilating the absolute LAI values derived from remote sensing (mean R2 = 0.30, RMSE = 1257/ha). Our findings demonstrated a reliable and promising assimilation scheme for improving yield estimation of crop model at regional scale with low data requirement.},
  keywords = {Data assimilation,Leaf area index (LAI),MCWLA-Wheat model,Spatial differences,Winter wheat,Yield estimation}
}

@misc{chen_vlp_2022,
  title = {{{VLP}}: {{A Survey}} on {{Vision-Language Pre-training}}},
  shorttitle = {{{VLP}}},
  author = {Chen, Feilong and Zhang, Duzhen and Han, Minglun and Chen, Xiuyi and Shi, Jing and Xu, Shuang and Xu, Bo},
  year = {2022},
  month = jul,
  number = {arXiv:2202.09061},
  eprint = {2202.09061},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {In the past few years, the emergence of pre-training models has brought uni-modal fields such as computer vision (CV) and natural language processing (NLP) to a new era. Substantial works have shown they are beneficial for downstream uni-modal tasks and avoid training a new model from scratch. So can such pre-trained models be applied to multimodal tasks? Researchers have explored this problem and made significant progress. This paper surveys recent advances and new frontiers in visionlanguage pre-training (VLP), including image-text and video-text pre-training. To give readers a better overall grasp of VLP, we first review its recent advances from five aspects: feature extraction, model architecture, pre-training objectives, pre-training datasets, and downstream tasks. Then, we summarize the specific VLP models in detail. Finally, we discuss the new frontiers in VLP. To the best of our knowledge, this is the first survey on VLP. We hope that this survey can shed light on future research in the VLP field.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\HP\\Zotero\\storage\\UYYN5ZW2\\Chen 等 - 2022 - VLP A Survey on Vision-Language Pre-training.pdf}
}

@article{chu_end--end_2020,
  title = {An End-to-End Model for Rice Yield Prediction Using Deep Learning Fusion},
  author = {Chu, Zheng and Yu, Jiong},
  year = {2020},
  journal = {Computers and Electronics in Agriculture},
  volume = {174},
  pages = {105471},
  issn = {0168-1699},
  doi = {10.1016/j.compag.2020.105471},
  abstract = {Rice yield is essential for more than half of the world's population, and thus, accurate predictions of rice yield are of great importance for trade, development policies, humanitarian assistance, decision-makers, etc. However, traditional mechanistic models and statistical machine learning models need to identify features, making the research on and application of these models laborious and time-consuming. In this paper, a novel end-to-end prediction model that fuses two back-propagation neural networks (BPNNs) with an independently recurrent neural network (IndRNN), named BBI-model, is proposed to address these challenges. In stage one, BBI-model preprocesses the original area and meteorology data. In stage two, one BPNN and the IndRNN are used to learn deep spatial and temporal features in parallel. In stage three, another BPNN combines two kinds of deep features and learns the relationships between these deep features and rice yields to make predictions for summer and winter rice yields. The experimental results indicate that BBI-model achieved the lowest mean absolute error (MAE) and root mean square error (RMSE) for the summer rice prediction (0.0044 and 0.0057, respectively) and corresponding values of 0.0074 and 0.0192 for the winter rice prediction when the number of layers in the network was set to six. Moreover, the errors of the model using the combination of deep spatial-temporal features were significantly lower than when simply using deep temporal features. Furthermore, the model converged quickly with 100 iterations and then remained stable. These findings confirm that the model can make accurate predictions for summer and winter rice yields of 81 counties in the Guangxi Zhuang Autonomous Region, China.},
  keywords = {Deep learning fusion,Deep spatial features,Deep temporal features,End-to-end model,Rice,Yield prediction}
}

@article{clevers_remote_2013,
  title = {Remote Estimation of Crop and Grass Chlorophyll and Nitrogen Content Using Red-Edge Bands on {{Sentinel-2}} and-3},
  author = {Clevers, Jan GPW and Gitelson, Anatoly A},
  year = {2013},
  journal = {International Journal of Applied Earth Observation and Geoinformation},
  volume = {23},
  pages = {344--351},
  publisher = {{Elsevier}}
}

@misc{cukurova_promise_2020,
  title = {The Promise and Challenges of Multimodal Learning Analytics},
  author = {Cukurova, Mutlu and Giannakos, Michail and {Martinez-Maldonado}, Roberto},
  year = {2020},
  journal = {British Journal of Educational Technology},
  volume = {51},
  number = {5},
  pages = {1441--1449},
  publisher = {{Wiley Online Library}}
}

@article{dahikar_agricultural_2014,
  title = {Agricultural Crop Yield Prediction Using Artificial Neural Network Approach},
  author = {Dahikar, Snehal S and Rode, Sandeep V},
  year = {2014},
  journal = {International journal of innovative research in electrical, electronics, instrumentation and control engineering},
  volume = {2},
  number = {1},
  pages = {683--686},
  publisher = {{Citeseer}}
}

@inproceedings{deng_imagenet_2009,
  title = {Imagenet: {{A}} Large-Scale Hierarchical Image Database},
  booktitle = {2009 {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and {Fei-Fei}, Li},
  year = {2009},
  pages = {248--255},
  publisher = {{Ieee}}
}

@misc{dosovitskiy_image_2021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk},
  year = {2021},
  month = jun,
  number = {arXiv:2010.11929},
  eprint = {2010.11929},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\HP\\Zotero\\storage\\ULUI66ZE\\Dosovitskiy 等 - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf}
}

@inproceedings{drummond_application_1998,
  title = {Application of Neural Networks: Precision Farming},
  booktitle = {1998 {{IEEE International Joint Conference}} on {{Neural Networks Proceedings}}. {{IEEE World Congress}} on {{Computational Intelligence}} ({{Cat}}. {{No}}. {{98CH36227}})},
  author = {Drummond, Scott and Joshi, Anupam and Sudduth, Kenneth A},
  year = {1998},
  volume = {1},
  pages = {211--215},
  publisher = {{IEEE}}
}

@article{ehret_neural_2011,
  title = {Neural Network Modeling of Greenhouse Tomato Yield, Growth and Water Use from Automated Crop Monitoring Data},
  author = {Ehret, David L and Hill, Bernard D and Helmer, Tom and Edwards, Diane R},
  year = {2011},
  journal = {Computers and electronics in agriculture},
  volume = {79},
  number = {1},
  pages = {82--89},
  publisher = {{Elsevier}}
}

@article{fortin_site-specific_2011,
  title = {Site-Specific Early Season Potato Yield Forecast by Neural Network in {{Eastern Canada}}},
  author = {Fortin, J{\'e}r{\^o}me G and Anctil, Fran{\c c}ois and Parent, L{\'e}on-{\'E}tienne and Bolinder, Martin A},
  year = {2011},
  journal = {Precision agriculture},
  volume = {12},
  number = {6},
  pages = {905--923},
  publisher = {{Springer}}
}

@article{franch_improving_2015,
  title = {Improving the Timeliness of Winter Wheat Production Forecast in the {{United States}} of {{America}}, {{Ukraine}} and {{China}} Using {{MODIS}} Data and {{NCAR Growing Degree Day}} Information},
  author = {Franch, Belen and Vermote, EF and {Becker-Reshef}, Inbal and Claverie, Martin and Huang, Jianxi and Zhang, Jinshui and Justice, Chris and Sobrino, Jose Antonio},
  year = {2015},
  journal = {Remote Sensing of Environment},
  volume = {161},
  pages = {131--148},
  publisher = {{Elsevier}}
}

@misc{huang_seeing_2021,
  title = {Seeing {{Out}} of {{tHe bOx}}: {{End-to-End Pre-training}} for {{Vision-Language Representation Learning}}},
  shorttitle = {Seeing {{Out}} of {{tHe bOx}}},
  author = {Huang, Zhicheng and Zeng, Zhaoyang and Huang, Yupan and Liu, Bei and Fu, Dongmei and Fu, Jianlong},
  year = {2021},
  month = apr,
  number = {arXiv:2104.03135},
  eprint = {2104.03135},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We study joint learning of Convolutional Neural Network (CNN) and Transformer for vision-language pre-training (VLPT) which aims to learn cross-modal alignments from millions of image-text pairs. State-of-the-art approaches extract salient image regions and align regions with words step-by-step. As region-based visual features usually represent parts of an image, it is challenging for existing visionlanguage models to fully understand the semantics from paired natural languages. In this paper, we propose SOHO to ``See Out of tHe bOx'' that takes a whole image as input, and learns vision-language representation in an endto-end manner. SOHO does not require bounding box annotations which enables inference 10 times faster than regionbased approaches. In particular, SOHO learns to extract comprehensive yet compact image features through a visual dictionary (VD) that facilitates cross-modal understanding. VD is designed to represent consistent visual abstractions of similar semantics. It is updated on-the-fly and utilized in our proposed pre-training task Masked Visual Modeling (MVM). We conduct experiments on four well-established vision-language tasks by following standard VLPT settings. In particular, SOHO achieves absolute gains of 2.0\% R@1 score on MSCOCO text retrieval 5k test split, 1.5\% accuracy on NLVR2 test-P split, 6.7\% accuracy on SNLI-VE test split, respectively.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\HP\\Zotero\\storage\\GRCXTN5W\\Huang 等 - 2021 - Seeing Out of tHe bOx End-to-End Pre-training for(翻译结果).docx;C\:\\Users\\HP\\Zotero\\storage\\UI3QMDXL\\Huang 等 - 2021 - Seeing Out of tHe bOx End-to-End Pre-training for.pdf}
}

@misc{huang_what_2021,
  title = {What {{Makes Multi-modal Learning Better}} than {{Single}} ({{Provably}})},
  author = {Huang, Yu and Du, Chenzhuang and Xue, Zihui and Chen, Xuanyao and Zhao, Hang and Huang, Longbo},
  year = {2021},
  month = oct,
  number = {arXiv:2106.04538},
  eprint = {2106.04538},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {The world provides us with data of multiple modalities. Intuitively, models fusing data from different modalities outperform their uni-modal counterparts, since more information is aggregated. Recently, joining the success of deep learning, there is an influential line of work on deep multi-modal learning, which has remarkable empirical results on various applications. However, theoretical justifications in this field are notably lacking.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\HP\\Zotero\\storage\\9UUL4Z3T\\Huang 等 - 2021 - What Makes Multi-modal Learning Better than Single.pdf;C\:\\Users\\HP\\Zotero\\storage\\WRY2DF5P\\Huang 等 - 2021 - What Makes Multi-modal Learning Better than Single(翻译结果).docx}
}

@article{ines_assimilation_2013,
  title = {Assimilation of Remotely Sensed Soil Moisture and Vegetation with a Crop Simulation Model for Maize Yield Prediction},
  author = {Ines, Amor V. M. and Das, Narendra N. and Hansen, James W. and Njoku, Eni G.},
  year = {2013},
  journal = {Remote Sensing of Environment},
  volume = {138},
  pages = {149--164},
  issn = {00344257},
  doi = {10.1016/j.rse.2013.07.018}
}

@article{ines_assimilation_2013-1,
  title = {Assimilation of Remotely Sensed Soil Moisture and Vegetation with a Crop Simulation Model for Maize Yield Prediction},
  author = {Ines, Amor V.M. and Das, Narendra N. and Hansen, James W. and Njoku, Eni G.},
  year = {2013},
  month = nov,
  journal = {Remote Sensing of Environment},
  volume = {138},
  pages = {149--164},
  issn = {00344257},
  doi = {10.1016/j.rse.2013.07.018},
  abstract = {To improve the prediction of crop yields at an aggregate scale, we developed a data assimilation-crop modeling framework that incorporates remotely sensed soil moisture and leaf area index (LAI) into a crop model using sequential data assimilation. The core of the framework is an Ensemble Kalman Filter (EnKF) used to control crop model runs, assimilate remote sensing (RS) data and update model state variables. We modified the Decision Support System for Agro-technology Transfer \textendash{} Cropping System Model (DSSAT-CSM)-Maize model (Jones et al., 2003) to be able to stop and start simulations at any given time in the growing season, such that the EnKF can update model state variables as RS data become available. The data assimilation-crop modeling framework was evaluated against 2003\textendash 2009 maize yields in Story County, Iowa, USA, assimilating AMSR-E soil moisture and MODIS-LAI data independently and simultaneously. Assimilating LAI or soil moisture independently slightly improved the correlation of observed and simulated yields (R = 0.51 and 0.50) compared to no data assimilation (open-loop; R = 0.47) but prediction errors improved with reductions in MBE and RMSE by 0.5 and 0.5 Mg ha-1 respectively for LAI assimilation while these were reduced by 1.8 and 1.1 Mg ha-1 for soil moisture assimilation. Yield correlation improved more when both soil moisture and LAI were assimilated (R = 0.65) suggesting a cause\textendash effect interaction between soil moisture and LAI, prediction errors (MBE and RMSE) were also reduced by 1.7 and 1.8 Mg ha-1 with respect to open-loop simulations. Results suggest that assimilation of LAI independently might be preferable when conditions are extremely wet while assimilation of soil moisture + LAI might be more suitable when conditions are more nominal. AMSR-E soil moisture tends to be more biased under the presence of high vegetation (i.e., when crops are fully developed) and that updating rootzone soil moisture by near-surface soil moisture assimilation under very wet conditions could increase the modeled percolation causing excessive nitrogen (N) leaching hence reducing crop yields even with water stress reduced at a minimum due to soil moisture assimilation. However, applying the data assimilation-crop modeling framework strategically by considering a-priori information on climate condition expected during the growing season may improve yield prediction performance substantially, in our case with higher correlation (R = 0.80) and more reductions in MBE and RMSE (2.5 and 3.3 Mg ha-1) compared to when there is no data assimilation. Scaling AMSR-E soil moisture to the climatology of the model did not improve our data assimilation results because the model is also biased. Better soil moisture products e.g., from Soil Moisture Active Passive (SMAP) mission, may solve the soil moisture data issue in the near future.},
  langid = {english},
  file = {C\:\\Users\\HP\\Zotero\\storage\\6R6HCGEH\\Ines 等 - 2013 - Assimilation of remotely sensed soil moisture and .pdf}
}

@inproceedings{islam_deep_2018,
  title = {A {{Deep Neural Network Approach}} for {{Crop Selection}} and {{Yield Prediction}} in {{Bangladesh}}},
  booktitle = {2018 {{IEEE Region}} 10 {{Humanitarian Technology Conference}} ({{R10-HTC}})},
  author = {Islam, Tanhim and Chisty, Tanjir Alam and Chakrabarty, Amitabha},
  year = {2018},
  pages = {1--6},
  doi = {10.1109/R10-HTC.2018.8629828},
  file = {C\:\\Users\\HP\\Zotero\\storage\\BS7R6MZS\\Islam 等 - 2018 - A Deep Neural Network Approach for Crop Selection .pdf;C\:\\Users\\HP\\Zotero\\storage\\DRTWIICI\\Islam 等 - 2018 - A Deep Neural Network Approach for Crop Selection (翻译结果).docx}
}

@article{jin_smallholder_2019,
  title = {Smallholder Maize Area and Yield Mapping at National Scales with {{Google Earth Engine}}},
  author = {Jin, Zhenong and Azzari, George and You, Calum and Di Tommaso, Stefania and Aston, Stephen and Burke, Marshall and Lobell, David B},
  year = {2019},
  journal = {Remote Sensing of Environment},
  volume = {228},
  pages = {115--128},
  publisher = {{Elsevier}}
}

@article{johnson_crop_2016,
  title = {Crop Yield Forecasting on the {{Canadian Prairies}} by Remotely Sensed Vegetation Indices and Machine Learning Methods},
  author = {Johnson, Michael D and Hsieh, William W and Cannon, Alex J and Davidson, Andrew and B{\'e}dard, Fr{\'e}d{\'e}ric},
  year = {2016},
  journal = {Agricultural and forest meteorology},
  volume = {218},
  pages = {74--84},
  publisher = {{Elsevier}}
}

@article{johnson_crop_2016-1,
  title = {Crop Yield Forecasting on the {{Canadian Prairies}} by Remotely Sensed Vegetation Indices and Machine Learning Methods},
  author = {Johnson, Michael D and Hsieh, William W and Cannon, Alex J and Davidson, Andrew and B{\'e}dard, Fr{\'e}d{\'e}ric},
  year = {2016},
  journal = {Agricultural and forest meteorology},
  volume = {218},
  pages = {74--84},
  publisher = {{Elsevier}}
}

@article{kalaiarasi_multi-parametric_2022,
  title = {Multi-Parametric Multiple Kernel Deep Neural Network for Crop Yield Prediction},
  author = {Kalaiarasi, E. and Anbarasi, A.},
  year = {2022},
  journal = {Materials Today: Proceedings},
  volume = {62},
  pages = {4635--4642},
  issn = {2214-7853},
  doi = {10.1016/j.matpr.2022.03.115},
  abstract = {A rapid, precise and credible prediction of crop yield at a wider scale is more important than ever for crop management, the measurement of food production, food trading and policymaking to address the challenges of environmental change, increased population and food demand. Deep Learning (DL) models are recently well-known for predicting crop yields. Multi-parametric Deep Neural Network (MDNN) is a DL model employed to estimate the crop yield concerning multiple parameters such as climate and soil. A Growing-Degree Day (GDD) has been used to determine the impact of climate change on crop yield. The determined values along with the climate and soil parameters have been learned by the DNN to estimate the yield quality. The MDNN performs well with the huge volume of data and it is not suitable for the medium scale of data. The learning ability of MDNN is improved in this paper by proposing a Multi-parametric Multiple Kernel DNN (MMKDNN) to provide better crop yield prediction for medium-scale data. The effectiveness of the model built in the last hidden layer is solely determined by the intermediate representations of the input. The intermediate representation of the input in a neural network is combined through multiple kernel learning. The MMKDNN with increasing complexity representations is preserved in this way, but the output calculation, i.e. crop yield prediction, is free to use all of the network's knowledge. For five different types of crops, the experiments are conducted to assess the efficiency of the MMKDNN.},
  keywords = {Crop yield prediction,DNN,GDD,Multi-parametric multiple kernel DNN,Multiple kernel learning}
}

@inproceedings{kamilaris_agri-iot_2016,
  title = {Agri-{{IoT}}: {{A}} Semantic Framework for {{Internet}} of {{Things-enabled}} Smart Farming Applications},
  booktitle = {2016 {{IEEE}} 3rd {{World Forum}} on {{Internet}} of {{Things}} ({{WF-IoT}})},
  author = {Kamilaris, Andreas and Gao, Feng and {Prenafeta-Boldu}, Francesc X. and Ali, Muhammad Intizar},
  year = {2016},
  pages = {442--447},
  doi = {10.1109/WF-IoT.2016.7845467}
}

@article{kamilaris_deep_2018,
  title = {Deep Learning in Agriculture: {{A}} Survey},
  author = {Kamilaris, Andreas and {Prenafeta-Bold{\'u}}, Francesc X},
  year = {2018},
  journal = {Computers and electronics in agriculture},
  volume = {147},
  pages = {70--90},
  publisher = {{Elsevier}}
}

@inproceedings{kataoka_crop_2003,
  title = {Crop Growth Estimation System Using Machine Vision},
  author = {Kataoka, T. and Kaneko, T. and Okamoto, H. and Hata, S.},
  year = {2003},
  volume = {2},
  pages = {b1079-b1083 vol.2},
  publisher = {{IEEE}},
  doi = {10.1109/AIM.2003.1225492},
  abstract = {According to the philosophy of Precision Farming, the status of crops during their growing stages is important information for crop cultivation tasks and management. The system which was developed in this research involves the vegetation cover area of plant being determined by the vision system and the image processing technique, and the crop status, i.e., the plant height, the leaf length, and the dry matter, being estimated with the specific functions. The specific functions which show the relationship between the vegetation cover area of plants and the measured actual plant dimensions were analyzed using a growth curve (the Gompertz curve) and an exponential function. The Gompertz curve was used for the estimation of the dry mass of the plants. For the leaf length and the plant height, the exponential function worked well compared to the growth curve. Based on the results, the crop growing status could be estimated using crop images and calculated equations.},
  keywords = {Agriculture,Area measurement,Crops,Equations,Image processing,Lenses,Machine vision,Sugar industry,Testing,Vegetation mapping}
}

@inproceedings{kataoka_crop_2003-1,
  title = {Crop Growth Estimation System Using Machine Vision},
  booktitle = {Proceedings 2003 {{IEEE}}/{{ASME International Conference}} on {{Advanced Intelligent Mechatronics}} ({{AIM}} 2003)},
  author = {Kataoka, T. and Kaneko, T. and Okamoto, H. and Hata, S.},
  year = {2003},
  volume = {2},
  pages = {b1079-b1083},
  publisher = {{IEEE}},
  address = {{Kobe, Japan}},
  doi = {10.1109/AIM.2003.1225492},
  abstract = {According to the philosophy of Precision Farming, the status of crops during their growing stages is important informationfor crop cultivation tasks and management.},
  isbn = {978-0-7803-7759-2},
  langid = {english},
  file = {C\:\\Users\\HP\\Zotero\\storage\\SA452942\\Kataoka 等 - 2003 - Crop growth estimation system using machine vision.pdf}
}

@article{khan_transformers_2022,
  title = {Transformers in Vision: {{A}} Survey},
  author = {Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  year = {2022},
  journal = {ACM computing surveys (CSUR)},
  volume = {54},
  number = {10s},
  pages = {1--41},
  publisher = {{ACM New York, NY}}
}

@misc{kim_vilt_2021,
  title = {{{ViLT}}: {{Vision-and-Language Transformer Without Convolution}} or {{Region Supervision}}},
  shorttitle = {{{ViLT}}},
  author = {Kim, Wonjae and Son, Bokyung and Kim, Ildoo},
  year = {2021},
  month = jun,
  number = {arXiv:2102.03334},
  eprint = {2102.03334},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Vision-and-Language Pre-training (VLP) has improved performance on various joint vision-and-language downstream tasks. Current approaches to VLP heavily rely on image feature extraction processes, most of which involve region supervision (e.g., object detection) and the convolutional architecture (e.g., ResNet). Although disregarded in the literature, we find it problematic in terms of both (1) efficiency/speed, that simply extracting input features requires much more computation than the multimodal interaction steps; and (2) expressive power, as it is upper bounded to the expressive power of the visual embedder and its predefined visual vocabulary. In this paper, we present a minimal VLP model, Vision-and-Language Transformer (ViLT), monolithic in the sense that the processing of visual inputs is drastically simplified to just the same convolution-free manner that we process textual inputs. We show that ViLT is up to tens of times faster than previous VLP models, yet with competitive or better downstream task performance. Our code and pre-trained weights are available at https://github.com/dandelin/vilt.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\HP\\Zotero\\storage\\CL54XND3\\ViLT_Vision-and-Language_Transformer_Without_Convolution_or_Region_Supervision_compressed zh.pdf;C\:\\Users\\HP\\Zotero\\storage\\WHB6IRXW\\Kim 等 - 2021 - ViLT Vision-and-Language Transformer Without Conv.pdf}
}

@article{kitano_corn_2019,
  title = {Corn {{Plant Counting Using Deep Learning}} and {{UAV Images}}},
  author = {Kitano, Bruno T. and Mendes, Caio C. T. and Geus, Andre R. and Oliveira, Henrique C. and Souza, Jefferson R.},
  year = {2019},
  journal = {IEEE Geoscience and Remote Sensing Letters},
  pages = {1--5},
  issn = {1545-598X, 1558-0571},
  doi = {10.1109/LGRS.2019.2930549},
  abstract = {The adoption of new technologies, such as unmanned aerial vehicles (UAVs), image processing, and machine learning, is disrupting traditional concepts in agriculture, with a new range of possibilities opening in its fields of research. Plant density is one of the most important corn (Zea mays L.) yield factors, yet its precise measurement after the emergence of plants is impractical in large-scale production fields due to the amount of labor required. This letter aims to develop techniques that enable corn plant counting and the automation of this process through deep learning and computational vision, using images of several corn crops obtained using a low-cost unmanned aerial vehicle (UAV) platform assembled with an RGB sensor.},
  langid = {english},
  file = {C\:\\Users\\HP\\Zotero\\storage\\Q4GS3D4D\\Kitano 等 - 2019 - Corn Plant Counting Using Deep Learning and UAV Im.pdf}
}

@article{kitchen_soil_2003,
  title = {Soil Electrical Conductivity and Topography Related to Yield for Three Contrasting Soil\textendash Crop Systems},
  author = {Kitchen, NR and Drummond, ST and Lund, ED and Sudduth, KA and Buchleiter, GW},
  year = {2003},
  journal = {Agronomy journal},
  volume = {95},
  number = {3},
  pages = {483--495},
  publisher = {{Wiley Online Library}}
}

@article{lecun_deep_2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  journal = {nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  publisher = {{Nature Publishing Group}}
}

@article{lecun_deep_2015-1,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  journal = {nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  publisher = {{Nature Publishing Group}}
}

@article{lei_video_2021,
  title = {Video Captioning Based on Channel Soft Attention and Semantic Reconstructor},
  author = {Lei, Zhou and Huang, Yiyong},
  year = {2021},
  journal = {Future Internet},
  volume = {13},
  number = {2},
  pages = {55},
  publisher = {{MDPI}}
}

@misc{li_pose_2021,
  title = {Pose {{Recognition}} with {{Cascade Transformers}}},
  author = {Li, Ke and Wang, Shijie and Zhang, Xiang and Xu, Yifan and Xu, Weijian and Tu, Zhuowen},
  year = {2021},
  month = apr,
  number = {arXiv:2104.06976},
  eprint = {2104.06976},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {In this paper, we present a regression-based pose recognition method using cascade Transformers. One way to categorize the existing approaches in this domain is to separate them into 1). heatmap-based and 2). regressionbased. In general, heatmap-based methods achieve higher accuracy but are subject to various heuristic designs (not end-to-end mostly), whereas regression-based approaches attain relatively lower accuracy but they have less intermediate non-differentiable steps. Here we utilize the encoderdecoder structure in Transformers to perform regressionbased person and keypoint detection that is general-purpose and requires less heuristic design compared with the existing approaches. We demonstrate the keypoint hypothesis (query) refinement process across different self-attention layers to reveal the recursive self-attention mechanism in Transformers. In the experiments, we report competitive results for pose recognition when compared with the competing regression-based methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\HP\\Zotero\\storage\\U5ZQ7KGB\\Li 等 - 2021 - Pose Recognition with Cascade Transformers.pdf}
}

@misc{li_visualbert_2019,
  title = {{{VisualBERT}}: {{A Simple}} and {{Performant Baseline}} for {{Vision}} and {{Language}}},
  shorttitle = {{{VisualBERT}}},
  author = {Li, Liunian Harold and Yatskar, Mark and Yin, Da and Hsieh, Cho-Jui and Chang, Kai-Wei},
  year = {2019},
  month = aug,
  number = {arXiv:1908.03557},
  eprint = {1908.03557},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We propose VisualBERT, a simple and flexible framework for modeling a broad range of vision-and-language tasks. VisualBERT consists of a stack of Transformer layers that implicitly align elements of an input text and regions in an associated input image with self-attention. We further propose two visually-grounded language model objectives for pre-training VisualBERT on image caption data. Experiments on four vision-and-language tasks including VQA, VCR, NLVR2, and Flickr30K show that VisualBERT outperforms or rivals with state-of-the-art models while being significantly simpler. Further analysis demonstrates that VisualBERT can ground elements of language to image regions without any explicit supervision and is even sensitive to syntactic relationships, tracking, for example, associations between verbs and image regions corresponding to their arguments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\HP\\Zotero\\storage\\5T4CDBGT\\Li 等 - 2019 - VisualBERT A Simple and Performant Baseline for V.pdf}
}

@article{lin_survey_2022,
  title = {A Survey of Transformers},
  author = {Lin, Tianyang and Wang, Yuxin and Liu, Xiangyang and Qiu, Xipeng},
  year = {2022},
  journal = {AI Open},
  publisher = {{Elsevier}}
}

@article{liu_neural_2001,
  title = {A Neural Network for Setting Target Corn Yields},
  author = {Liu, Jing and Goering, CE and Tian, Lei},
  year = {2001},
  journal = {Transactions of the ASAE},
  volume = {44},
  number = {3},
  pages = {705},
  publisher = {{American Society of Agricultural and Biological Engineers}}
}

@article{lobell_scalable_2015,
  title = {A Scalable Satellite-Based Crop Yield Mapper},
  author = {Lobell, David B and Thau, David and Seifert, Christopher and Engle, Eric and Little, Bertis},
  year = {2015},
  journal = {Remote Sensing of Environment},
  volume = {164},
  pages = {324--333},
  publisher = {{Elsevier}}
}

@article{long_improving_2021,
  title = {Improving Reasoning with Contrastive Visual Information for Visual Question Answering},
  author = {Long, Yu and Tang, Pengjie and Wang, Hanli and Yu, Jian},
  year = {2021},
  journal = {Electronics Letters},
  volume = {57},
  number = {20},
  pages = {758--760},
  publisher = {{Wiley Online Library}}
}

@article{maimaitijiang_soybean_2020,
  title = {Soybean Yield Prediction from {{UAV}} Using Multimodal Data Fusion and Deep Learning},
  author = {Maimaitijiang, Maitiniyazi and Sagan, Vasit and Sidike, Paheding and Hartling, Sean and Esposito, Flavio and Fritschi, Felix B.},
  year = {2020},
  month = feb,
  journal = {Remote Sensing of Environment},
  volume = {237},
  pages = {111599},
  issn = {00344257},
  doi = {10.1016/j.rse.2019.111599},
  langid = {english},
  file = {C\:\\Users\\HP\\Zotero\\storage\\9HGHRV9V\\1-s2.0-S0034425719306194-main.pdf;C\:\\Users\\HP\\Zotero\\storage\\ZIJWEEEA\\1-s2.0-S0034425719306194-main(翻译结果).docx}
}

@article{maimaitijiang_unmanned_2017,
  title = {Unmanned {{Aerial System}} ({{UAS}})-Based Phenotyping of Soybean Using Multi-Sensor Data Fusion and Extreme Learning Machine},
  author = {Maimaitijiang, Maitiniyazi and Ghulam, Abduwasit and Sidike, Paheding and Hartling, Sean and Maimaitiyiming, Matthew and Peterson, Kyle and Shavers, Ethan and Fishman, Jack and Peterson, Jim and Kadam, Suhas and Burken, Joel and Fritschi, Felix},
  year = {2017},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {134},
  pages = {43--58},
  issn = {0924-2716},
  doi = {10.1016/j.isprsjprs.2017.10.011},
  abstract = {Estimating crop biophysical and biochemical parameters with high accuracy at low-cost is imperative for high-throughput phenotyping in precision agriculture. Although fusion of data from multiple sensors is a common application in remote sensing, less is known on the contribution of low-cost RGB, multispectral and thermal sensors to rapid crop phenotyping. This is due to the fact that (1) simultaneous collection of multi-sensor data using satellites are rare and (2) multi-sensor data collected during a single flight have not been accessible until recent developments in Unmanned Aerial Systems (UASs) and UAS-friendly sensors that allow efficient information fusion. The objective of this study was to evaluate the power of high spatial resolution RGB, multispectral and thermal data fusion to estimate soybean (Glycine max) biochemical parameters including chlorophyll content and nitrogen concentration, and biophysical parameters including Leaf Area Index (LAI), above ground fresh and dry biomass. Multiple low-cost sensors integrated on UASs were used to collect RGB, multispectral, and thermal images throughout the growing season at a site established near Columbia, Missouri, USA. From these images, vegetation indices were extracted, a Crop Surface Model (CSM) was advanced, and a model to extract the vegetation fraction was developed. Then, spectral indices/features were combined to model and predict crop biophysical and biochemical parameters using Partial Least Squares Regression (PLSR), Support Vector Regression (SVR), and Extreme Learning Machine based Regression (ELR) techniques. Results showed that: (1) For biochemical variable estimation, multispectral and thermal data fusion provided the best estimate for nitrogen concentration and chlorophyll (Chl) a content (RMSE of 9.9\% and 17.1\%, respectively) and RGB color information based indices and multispectral data fusion exhibited the largest RMSE 22.6\%; the highest accuracy for Chl a+b content estimation was obtained by fusion of information from all three sensors with an RMSE of 11.6\%. (2) Among the plant biophysical variables, LAI was best predicted by RGB and thermal data fusion while multispectral and thermal data fusion was found to be best for biomass estimation. (3) For estimation of the above mentioned plant traits of soybean from multi-sensor data fusion, ELR yields promising results compared to PLSR and SVR in this study. This research indicates that fusion of low-cost multiple sensor data within a machine learning framework can provide relatively accurate estimation of plant traits and provide valuable insight for high spatial precision in agriculture and plant stress assessment.},
  keywords = {Data Fusion,Extreme Learning Machine (ELM),Extreme Learning Machine based Regression (ELR),Phenotyping,Remote sensing,Unmanned Aerial System (UAS)}
}

@article{matsumura_maize_2015,
  title = {Maize Yield Forecasting by Linear Regression and Artificial Neural Networks in {{Jilin}}, {{China}}},
  author = {Matsumura, Kanichiro and Gaitan, Carlos F and Sugimoto, Kenji and Cannon, Alex J and Hsieh, William W},
  year = {2015},
  journal = {The Journal of Agricultural Science},
  volume = {153},
  number = {3},
  pages = {399--410},
  publisher = {{Cambridge University Press}}
}

@article{morimoto_dynamic_2007,
  title = {Dynamic Optimization of Watering {{Satsuma}} Mandarin Using Neural Networks and Genetic Algorithms},
  author = {Morimoto, T and Ouchi, Y and Shimizu, M and Baloch, MS},
  year = {2007},
  journal = {Agricultural water management},
  volume = {93},
  number = {1-2},
  pages = {1--10},
  publisher = {{Elsevier}}
}

@article{muhammad_comprehensive_2021,
  title = {A Comprehensive Survey on Multimodal Medical Signals Fusion for Smart Healthcare Systems},
  author = {Muhammad, Ghulam and Alshehri, Fatima and Karray, Fakhri and Saddik, Abdulmotaleb El and Alsulaiman, Mansour and Falk, Tiago H.},
  year = {2021},
  month = dec,
  journal = {Information Fusion},
  volume = {76},
  pages = {355--375},
  issn = {15662535},
  doi = {10.1016/j.inffus.2021.06.007},
  abstract = {Smart healthcare is a framework that utilizes technologies such as wearable devices, the Internet of Medical Things (IoMT), sophisticated machine learning algorithms, and wireless communication technology to seam\- lessly access health records, link individuals, resources, and organizations, and then effectively handle and react to health environment demands intelligently. One of the main ingredients of smart healthcare is medical sensors or IoMT. Due to the complex nature of diseases, in many cases, there is a need for multimodal medical signals for their diagnoses. While using multimodal signals, the most important issue is how to fuse them \textendash{} an area of burgeoning interest within the research community. This paper presents a comprehensive survey of multimodal medical signals fusion schemes that have been proposed for smart healthcare applications. Research works included in major repositories, such as IEEE Xplore, Science Direct, Springer Link, and ACM digital library have been surveyed to address several related research questions. Focus is placed on recent developments, thus only works published between 2014-2020 are considered. Finally, key research challenges and possible future di\- rections are also provided.},
  langid = {english},
  file = {C\:\\Users\\HP\\Zotero\\storage\\8GI73NL5\\Muhammad 等 - 2021 - A comprehensive survey on multimodal medical signa.pdf}
}

@article{nagrani_attention_nodate,
  title = {Attention {{Bottlenecks}} for {{Multimodal Fusion}}},
  author = {Nagrani, Arsha and Yang, Shan and Arnab, Anurag and Jansen, Aren and Schmid, Cordelia and Sun, Chen},
  pages = {14},
  abstract = {Humans perceive the world by concurrently processing and fusing highdimensional inputs from multiple modalities such as vision and audio. Machine perception models, in stark contrast, are typically modality-specific and optimised for unimodal benchmarks, and hence late-stage fusion of final representations or predictions from each modality (`late-fusion') is still a dominant paradigm for multimodal video classification. Instead, we introduce a novel transformer based architecture that uses `fusion bottlenecks' for modality fusion at multiple layers. Compared to traditional pairwise self-attention, our model forces information between different modalities to pass through a small number of bottleneck latents, requiring the model to collate and condense relevant information in each modality and share what is necessary. We find that such a strategy improves fusion performance, at the same time reducing computational cost. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple audio-visual classification benchmarks including Audioset, Epic-Kitchens and VGGSound. All code and models will be released.},
  langid = {english},
  file = {C\:\\Users\\HP\\Zotero\\storage\\LI96VPX2\\Nagrani 等 - Attention Bottlenecks for Multimodal Fusion(翻译结果).docx;C\:\\Users\\HP\\Zotero\\storage\\V6KF3PYT\\Nagrani 等 - Attention Bottlenecks for Multimodal Fusion.pdf}
}

@article{nevavuori_crop_2019,
  title = {Crop Yield Prediction with Deep Convolutional Neural Networks},
  author = {Nevavuori, Petteri and Narra, Nathaniel and Lipping, Tarmo},
  year = {2019},
  journal = {Computers and electronics in agriculture},
  volume = {163},
  pages = {104859},
  publisher = {{Elsevier}},
  file = {C\:\\Users\\HP\\Zotero\\storage\\F5ZTHKSL\\Nevavuori 等 - 2019 - Crop yield prediction with deep convolutional neur(翻译结果).docx;C\:\\Users\\HP\\Zotero\\storage\\ITEWAXPL\\Nevavuori 等 - 2019 - Crop yield prediction with deep convolutional neur.pdf}
}

@article{nevavuori_crop_2020,
  title = {Crop {{Yield Prediction Using Multitemporal UAV Data}} and {{Spatio-Temporal Deep Learning Models}}},
  author = {Nevavuori, Petteri and Narra, Nathaniel and Linna, Petri and Lipping, Tarmo},
  year = {2020},
  journal = {Remote Sensing},
  volume = {12},
  number = {23},
  issn = {2072-4292},
  doi = {10.3390/rs12234000},
  abstract = {Unmanned aerial vehicle (UAV) based remote sensing is gaining momentum worldwide in a variety of agricultural and environmental monitoring and modelling applications. At the same time, the increasing availability of yield monitoring devices in harvesters enables input-target mapping of in-season RGB and crop yield data in a resolution otherwise unattainable by openly availabe satellite sensor systems. Using time series UAV RGB and weather data collected from nine crop fields in Pori, Finland, we evaluated the feasibility of spatio-temporal deep learning architectures in crop yield time series modelling and prediction with RGB time series data. Using Convolutional Neural Networks (CNN) and Long-Short Term Memory (LSTM) networks as spatial and temporal base architectures, we developed and trained CNN-LSTM, convolutional LSTM and 3D-CNN architectures with full 15 week image frame sequences from the whole growing season of 2018. The best performing architecture, the 3D-CNN, was then evaluated with several shorter frame sequence configurations from the beginning of the season. With 3D-CNN, we were able to achieve 218.9 kg/ha mean absolute error (MAE) and 5.51\% mean absolute percentage error (MAPE) performance with full length sequences. The best shorter length sequence performance with the same model was 292.8 kg/ha MAE and 7.17\% MAPE with four weekly frames from the beginning of the season.}
}

@article{noauthor__nodate,
  title = {多模态论文综述：知乎}
}

@article{noauthor__nodate-1,
  title = {综合评价中数据标准化方法比较研究}
}

@article{noauthor_review_nodate,
  title = {A {{Review}} on {{Methods}} and {{Applications}} in {{Multimodal Deep Learning}}}
}

@misc{noauthor_transformer_nodate,
  title = {介绍transformer模型参数的}
}

@article{oneal_neural_2002,
  title = {Neural Network Prediction of Maize Yield Using Alternative Data Coding Algorithms},
  author = {O'Neal, Monte R and Engel, Bernard A and Ess, Daniel R and Frankenberger, Jane R},
  year = {2002}
}

@article{padilla_proximal_2018,
  title = {Proximal Optical Sensors for Nitrogen Management of Vegetable Crops: {{A}} Review},
  author = {Padilla, Francisco M and Gallardo, Marisa and {Pe{\~n}a-Fleitas}, M Teresa and De Souza, Romina and Thompson, Rodney B},
  year = {2018},
  journal = {Sensors},
  volume = {18},
  number = {7},
  pages = {2083},
  publisher = {{MDPI}}
}

@article{panda_application_2010,
  title = {Application of Vegetation Indices for Agricultural Crop Yield Prediction Using Neural Network Techniques},
  author = {Panda, Sudhanshu Sekhar and Ames, Daniel P and Panigrahi, Suranjan},
  year = {2010},
  journal = {Remote sensing},
  volume = {2},
  number = {3},
  pages = {673--696},
  publisher = {{Molecular Diversity Preservation International}}
}

@misc{prakash_multi-modal_2021,
  title = {Multi-{{Modal Fusion Transformer}} for {{End-to-End Autonomous Driving}}},
  author = {Prakash, Aditya and Chitta, Kashyap and Geiger, Andreas},
  year = {2021},
  month = apr,
  number = {arXiv:2104.09224},
  eprint = {2104.09224},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {How should representations from complementary sensors be integrated for autonomous driving? Geometrybased sensor fusion has shown great promise for perception tasks such as object detection and motion forecasting. However, for the actual driving task, the global context of the 3D scene is key, e.g. a change in traffic light state can affect the behavior of a vehicle geometrically distant from that traffic light. Geometry alone may therefore be insufficient for effectively fusing representations in end-to-end driving models. In this work, we demonstrate that imitation learning policies based on existing sensor fusion methods under-perform in the presence of a high density of dynamic agents and complex scenarios, which require global contextual reasoning, such as handling traffic oncoming from multiple directions at uncontrolled intersections. Therefore, we propose TransFuser, a novel Multi-Modal Fusion Transformer, to integrate image and LiDAR representations using attention. We experimentally validate the efficacy of our approach in urban settings involving complex scenarios using the CARLA urban driving simulator. Our approach achieves state-of-the-art driving performance while reducing collisions by 76\% compared to geometry-based fusion.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\HP\\Zotero\\storage\\SQAXSDW8\\Prakash 等 - 2021 - Multi-Modal Fusion Transformer for End-to-End Auto(翻译结果).docx;C\:\\Users\\HP\\Zotero\\storage\\UQQ3UYCZ\\2104.09224 zh.pdf;C\:\\Users\\HP\\Zotero\\storage\\XQ2D2S4W\\Prakash 等 - 2021 - Multi-Modal Fusion Transformer for End-to-End Auto.pdf}
}

@article{qin_hyperspectral_2017,
  title = {Hyperspectral Estimation Model for Predicting {{LAI}} of Rice in {{Ningxia}} Irrigation Zone},
  author = {QIN, Zhanfei and SHEN, Jian and XIE, Baoni and YAN, Lin and CHANG, Qingrui},
  year = {2017},
  journal = {武汉大学学报\ding{108} 信息科学版},
  volume = {42},
  number = {8},
  pages = {1159--1166},
  publisher = {{武汉大学学报\ding{108} 信息科学版}}
}

@article{ramachandram_deep_2017,
  title = {Deep {{Multimodal Learning}}: {{A Survey}} on {{Recent Advances}} and {{Trends}}},
  shorttitle = {Deep {{Multimodal Learning}}},
  author = {Ramachandram, Dhanesh and Taylor, Graham W.},
  year = {2017},
  month = nov,
  journal = {IEEE Signal Processing Magazine},
  volume = {34},
  number = {6},
  pages = {96--108},
  issn = {1053-5888},
  doi = {10.1109/MSP.2017.2738401},
  abstract = {We have highlighted three major areas where deep multimodal learning approaches have gained a foothold and continue to experience rapid advancements. In addition to the work already highlighted with respect to each of these key areas, we list additional representative work involving deep multimodal learning in Table 3. Several other application areas that involve text, images, and video, e.g., visual question answering (VQA) and image and video annotations, are highlighted in subsequent sections where we discuss specific deep-learning models.},
  langid = {english},
  file = {C\:\\Users\\HP\\Zotero\\storage\\HUGXUM5Q\\Ramachandram 和 Taylor - 2017 - Deep Multimodal Learning A Survey on Recent Advan.pdf}
}

@article{rasti_survey_2022,
  title = {A Survey of High Resolution Image Processing Techniques for Cereal Crop Growth Monitoring},
  author = {Rasti, Sanaz and Bleakley, Chris J. and Holden, N. M. and Whetton, Rebecca and Langton, David and O Hare, Gregory},
  year = {2022},
  journal = {Information Processing in Agriculture},
  volume = {9},
  number = {2},
  pages = {300--315},
  issn = {22143173},
  doi = {10.1016/j.inpa.2021.02.005}
}

@article{rasti_survey_2022-1,
  title = {A Survey of High Resolution Image Processing Techniques for Cereal Crop Growth Monitoring},
  author = {Rasti, Sanaz and Bleakley, Chris J. and Holden, N.M. and Whetton, Rebecca and Langton, David and O'Hare, Gregory},
  year = {2022},
  month = jun,
  journal = {Information Processing in Agriculture},
  volume = {9},
  number = {2},
  pages = {300--315},
  issn = {22143173},
  doi = {10.1016/j.inpa.2021.02.005},
  abstract = {This paper presents a survey of image processing techniques proposed in the literature for extracting key cereal crop growth metrics from high spatial resolution, typically proximal images. The descriptive crop growth metrics considered are: crop canopy cover, above ground biomass, leaf area index (including green area index), chlorophyll content, and growth stage. The paper includes an overview of relevant fundamental image processing techniques including camera types, colour spaces, colour indexes, and image segmentation. The descriptive crop growth metrics are defined. Reference methods for groundtruth measurement are described. Image processing methods for metric estimation are described in detail. The performance of the methods is reviewed and compared. The survey reveals limitations in image processing techniques for cereal crop monitoring such as lack of robustness to lighting conditions, camera position, and self-obstruction. Directions for future research to improve performance are identified.},
  langid = {english},
  file = {C\:\\Users\\HP\\Zotero\\storage\\2KBFI7CY\\Rasti 等 - 2022 - A survey of high resolution image processing techn.pdf}
}

@article{ruan_survey_2022,
  title = {Survey: {{Transformer}} Based Video-Language Pre-Training},
  author = {Ruan, Ludan and Jin, Qin},
  year = {2022},
  journal = {AI Open},
  publisher = {{Elsevier}}
}

@article{sakamoto_near_2014,
  title = {Near Real-Time Prediction of {{US}} Corn Yields Based on Time-Series {{MODIS}} Data},
  author = {Sakamoto, Toshihiro and Gitelson, Anatoly A and Arkebauer, Timothy J},
  year = {2014},
  journal = {Remote Sensing of Environment},
  volume = {147},
  pages = {219--231},
  publisher = {{Elsevier}}
}

@article{saxena_survey_2014,
  title = {A Survey of Image Processing Techniques for Agriculture},
  author = {Saxena, Lalit and Armstrong, Leisa},
  year = {2014},
  publisher = {{Australian Society of Information and Communication Technologies in Agriculture}}
}

@article{selva_video_2022,
  title = {Video Transformers: {{A}} Survey},
  author = {Selva, Javier and Johansen, Anders S and Escalera, Sergio and Nasrollahi, Kamal and Moeslund, Thomas B and Clap{\'e}s, Albert},
  year = {2022},
  journal = {arXiv preprint arXiv:2201.05991},
  eprint = {2201.05991},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@article{sengupta_review_2020,
  title = {A Review of Deep Learning with Special Emphasis on Architectures, Applications and Recent Trends},
  author = {Sengupta, Saptarshi and Basak, Sanchita and Saikia, Pallabi and Paul, Sayak and Tsalavoutis, Vasilios and Atiah, Frederick and Ravi, Vadlamani and Peters, Alan},
  year = {2020},
  journal = {Knowledge-Based Systems},
  volume = {194},
  pages = {105596},
  publisher = {{Elsevier}}
}

@article{shamshad_transformers_2022,
  title = {Transformers in Medical Imaging: {{A}} Survey},
  author = {Shamshad, Fahad and Khan, Salman and Zamir, Syed Waqas and Khan, Muhammad Haris and Hayat, Munawar and Khan, Fahad Shahbaz and Fu, Huazhu},
  year = {2022},
  journal = {arXiv preprint arXiv:2201.09873},
  eprint = {2201.09873},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@article{smith_development_2005,
  title = {The Development of Embodied Cognition: {{Six}} Lessons from Babies},
  author = {Smith, Linda and Gasser, Michael},
  year = {2005},
  journal = {Artificial life},
  volume = {11},
  number = {1-2},
  pages = {13--29},
  publisher = {{MIT Press}}
}

@article{snoek_multimodal_2005,
  title = {Multimodal Video Indexing: {{A}} Review of the State-of-the-Art},
  author = {Snoek, Cees GM and Worring, Marcel},
  year = {2005},
  journal = {Multimedia tools and applications},
  volume = {25},
  number = {1},
  pages = {5--35},
  publisher = {{Springer}}
}

@article{souza_online_2021,
  title = {Online Multimedia Retrieval on {{CPU}}\textendash{{GPU}} Platforms with Adaptive Work Partition},
  author = {Souza, Rafael and Fernandes, Andr{\'e} and Teixeira, Thiago SFX and Teodoro, George and Ferreira, Renato},
  year = {2021},
  journal = {Journal of Parallel and Distributed Computing},
  volume = {148},
  pages = {31--45},
  publisher = {{Elsevier}}
}

@misc{summaira_review_2022,
  title = {A {{Review}} on {{Methods}} and {{Applications}} in {{Multimodal Deep Learning}}},
  author = {Summaira, Jabeen and Li, Xi and Shoib, Amin Muhammad and Abdul, Jabbar},
  year = {2022},
  month = feb,
  number = {arXiv:2202.09195},
  eprint = {2202.09195},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Deep Learning has implemented a wide range of applications and has become increasingly popular in recent years. The goal of multimodal deep learning (MMDL) is to create models that can process and link information using various modalities. Despite the extensive development made for unimodal learning, it still cannot cover all the aspects of human learning. Multimodal learning helps to understand and analyze better when various senses are engaged in the processing of information. This paper focuses on multiple types of modalities, i.e., image, video, text, audio, body gestures, facial expressions, and physiological signals. Detailed analysis of the baseline approaches and an in-depth study of recent advancements during the last five years (2017 to 2021) in multimodal deep learning applications has been provided. A fine-grained taxonomy of various multimodal deep learning methods is proposed, elaborating on different applications in more depth. Lastly, main issues are highlighted separately for each domain, along with their possible future research directions. CCS Concepts: \textbullet{} Computing methodologies \textrightarrow{} Machine learning; \textbullet{} Information systems \textrightarrow{} Multimedia and multimodal retrieval.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Multimedia},
  file = {C\:\\Users\\HP\\Zotero\\storage\\593DG2BP\\Summaira 等 - 2022 - A Review on Methods and Applications in Multimodal(翻译结果).docx;C\:\\Users\\HP\\Zotero\\storage\\E59S7CGS\\Summaira 等 - 2022 - A Review on Methods and Applications in Multimodal.pdf}
}

@article{tao_estimation_2020,
  title = {Estimation of {{Crop Growth Parameters Using UAV-Based Hyperspectral Remote Sensing Data}}},
  author = {Tao, Huilin and Feng, Haikuan and Xu, Liangji and Miao, Mengke and Long, Huiling and Yue, Jibo and Li, Zhenhai and Yang, Guijun and Yang, Xiaodong and Fan, Lingling},
  year = {2020},
  journal = {Sensors},
  volume = {20},
  number = {5},
  pages = {1296},
  issn = {1424-8220},
  doi = {10.3390/s20051296}
}

@article{tao_estimation_2020-1,
  title = {Estimation of {{Crop Growth Parameters Using UAV-Based Hyperspectral Remote Sensing Data}}},
  author = {Tao, Huilin and Feng, Haikuan and Xu, Liangji and Miao, Mengke and Long, Huiling and Yue, Jibo and Li, Zhenhai and Yang, Guijun and Yang, Xiaodong and Fan, Lingling},
  year = {2020},
  month = feb,
  journal = {Sensors},
  volume = {20},
  number = {5},
  pages = {1296},
  issn = {1424-8220},
  doi = {10.3390/s20051296},
  abstract = {Above-ground biomass (AGB) and the leaf area index (LAI) are important indicators for the assessment of crop growth, and are therefore important for agricultural management. Although improvements have been made in the monitoring of crop growth parameters using ground- and satellite-based sensors, the application of these technologies is limited by imaging difficulties, complex data processing, and low spatial resolution. Therefore, this study evaluated the use of hyperspectral indices, red-edge parameters, and their combination to estimate and map the distributions of AGB and LAI for various growth stages of winter wheat. A hyperspectral sensor mounted on an unmanned aerial vehicle was used to obtain vegetation indices and red-edge parameters, and stepwise regression (SWR) and partial least squares regression (PLSR) methods were used to accurately estimate the AGB and LAI based on these vegetation indices, red-edge parameters, and their combination. The results show that: (i) most of the studied vegetation indices and red-edge parameters are significantly highly correlated with AGB and LAI; (ii) overall, the correlations between vegetation indices and AGB and LAI, respectively, are stronger than those between red-edge parameters and AGB and LAI, respectively; (iii) Compared with the estimations using only vegetation indices or red-edge parameters, the estimation of AGB and LAI using a combination of vegetation indices and red-edge parameters is more accurate; and (iv) The estimations of AGB and LAI obtained using the PLSR method are superior to those obtained using the SWR method. Therefore, combining vegetation indices with red-edge parameters and using the PLSR method can improve the estimation of AGB and LAI.},
  langid = {english},
  file = {C\:\\Users\\HP\\Zotero\\storage\\TXVAB4YH\\Tao 等 - 2020 - Estimation of Crop Growth Parameters Using UAV-Bas.pdf}
}

@article{tay_efficient_2020,
  title = {Efficient Transformers: {{A}} Survey},
  author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  year = {2020},
  journal = {ACM Computing Surveys (CSUR)},
  publisher = {{ACM New York, NY}}
}

@article{tian_assessing_2011,
  title = {Assessing Newly Developed and Published Vegetation Indices for Estimating Rice Leaf Nitrogen Concentration with Ground-and Space-Based Hyperspectral Reflectance},
  author = {Tian, YC and Yao, X and Yang, J and Cao, WX and Hannaway, DB and Zhu, Y},
  year = {2011},
  journal = {Field Crops Research},
  volume = {120},
  number = {2},
  pages = {299--310},
  publisher = {{Elsevier}}
}

@article{tong_modelling_2016,
  title = {{Modelling the impacts of climate change on spring maize yield in Southwest China using the APSIM model}},
  author = {Tong, Dai},
  year = {2016},
  journal = {资源科学},
  volume = {38},
  number = {1},
  pages = {113--119},
  issn = {1007-7588},
  doi = {10.18402/resci.2016.01.17},
  langid = {chinese},
  file = {C\:\\Users\\HP\\Zotero\\storage\\8QGTZBNA\\Tong - 2016 - Modelling the impacts of climate change on spring .pdf}
}

@article{trnka_effect_2007,
  title = {Effect of {{Estimated Daily Global Solar Radiation Data}} on the {{Results}} of {{Crop Growth Models}}},
  author = {Trnka, Miroslav and Eitzinger, Josef and Kapler, Pavel and Dubrovsk{\'y}, Martin and Semer{\'a}dov{\'a}, Daniela and {\v Z}alud, Zden{\v e}k and Formayer, Herbert},
  year = {2007},
  month = oct,
  journal = {Sensors},
  volume = {7},
  number = {10},
  pages = {2330--2362},
  issn = {1424-8220},
  doi = {10.3390/s7102330},
  langid = {english},
  file = {C\:\\Users\\HP\\Zotero\\storage\\39HGQN5V\\Trnka 等 - 2007 - Effect of Estimated Daily Global Solar Radiation D.pdf;C\:\\Users\\HP\\Zotero\\storage\\UZQZGPXQ\\Trnka 等 - 2007 - Effect of Estimated Daily Global Solar Radiation D.pdf}
}

@article{turkoglu_crop_2021,
  title = {Crop Mapping from Image Time Series: {{Deep}} Learning with Multi-Scale Label Hierarchies},
  author = {Turkoglu, Mehmet Ozgur and D'Aronco, Stefano and Perich, Gregor and Liebisch, Frank and Streit, Constantin and Schindler, Konrad and Wegner, Jan Dirk},
  year = {2021},
  journal = {Remote Sensing of Environment},
  volume = {264},
  pages = {112603},
  issn = {0034-4257},
  doi = {10.1016/j.rse.2021.112603},
  abstract = {The aim of this paper is to map agricultural crops by classifying satellite image time series. Domain experts in agriculture work with crop type labels that are organised in a hierarchical tree structure, where coarse classes (like orchards) are subdivided into finer ones (like apples, pears, vines, etc.). We develop a crop classification method that exploits this expert knowledge and significantly improves the mapping of rare crop types. The three-level label hierarchy is encoded in a convolutional, recurrent neural network (convRNN), such that for each pixel the model predicts three labels at different level of granularity. This end-to-end trainable, hierarchical network architecture allows the model to learn joint feature representations of rare classes (e.g., apples, pears) at a coarser level (e.g., orchard), thereby boosting classification performance at the fine-grained level. Additionally, labelling at different granularity also makes it possible to adjust the output according to the classification scores; as coarser labels with high confidence are sometimes more useful for agricultural practice than fine-grained but very uncertain labels. We validate the proposed method on a new, large dataset that we make public. ZueriCrop covers an area of 50 km \texttimes{} 48 km in the Swiss cantons of Zurich and Thurgau with a total of 116{${'}$}000 individual fields spanning 48 crop classes, and 28,000 (multi-temporal) image patches from Sentinel-2. We compare our proposed hierarchical convRNN model with several baselines, including methods designed for imbalanced class distributions. The hierarchical approach performs superior by at least 9.9 percentage points in F1-score.},
  keywords = {Convolutional RNN,Crop classification,Deep learning,Hierarchical classification,Multi-stage,Multi-temporal,Recurrent neural network (RNN),Time series}
}

@inproceedings{vaswani_attention_2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}}
}

@article{wan_grain_2020,
  title = {Grain Yield Prediction of Rice Using Multi-Temporal {{UAV-based RGB}} and Multispectral Images and Model Transfer \textendash{} a Case Study of Small Farmlands in the {{South}} of {{China}}},
  author = {Wan, Liang and Cen, Haiyan and Zhu, Jiangpeng and Zhang, Jiafei and Zhu, Yueming and Sun, Dawei and Du, Xiaoyue and Zhai, Li and Weng, Haiyong and Li, Yijian and Li, Xiaoran and Bao, Yidan and Shou, Jianyao and He, Yong},
  year = {2020},
  journal = {Agricultural and Forest Meteorology},
  volume = {291},
  pages = {108096},
  issn = {0168-1923},
  doi = {10.1016/j.agrformet.2020.108096},
  abstract = {Timely and accurate crop monitoring and yield forecasting before harvesting are valuable for precision management, policy and decision making, and marketing. The aim of this study is to explore the potential of fusing spectral and structural information extracted from the unmanned aerial vehicle (UAV)-based images in the whole growth period of rice to improve the grain yield prediction. A UAV platform carrying RGB and multispectral cameras was employed to collect high spatial resolution images of the rice crop under different nitrogen treatments over two years. The vegetation indices (VIs), canopy height and canopy coverage were extracted from UAV-based images, which were then used to develop random forest prediction models for grain yield. Among all of the investigated VIs, it was found that normalized difference yellowness index (NDYI) was the most useful index to monitor the changes in leaf chlorophyll content as well as the leaf greenness during the whole growth period. Meanwhile, the VIs provided a comparable prediction of grain yield to field-measured aboveground biomass and leaf chlorophyll content. Fusion of the multi-temporal normalized difference vegetation index (NDVI), NDYI, canopy height and canopy coverage achieved the best prediction of grain yield with a determination coefficient of 0.85 and 0.83, and relative root mean square error of 3.56\% and 2.75\% in 2017 and 2018, respectively, which outperformed the results in the reported studies. The initial heading stage was the optimal growth stage for the prediction of grain yield. Furthermore, the robustness of prediction model developed from the dataset in 2017 was validated by an external dataset from 2018 using model transfer. These findings demonstrate that the proposed approach can improve the prediction accuracy of grain yield as well as achieve an efficient monitoring of crop growth.},
  keywords = {Canopy structural information,Data fusion,Grain yield,Model transfer,Unmanned aerial vehicle (UAV),Vegetation indices (VIs)}
}

@article{wang_new_2022,
  title = {New Image Dataset and New Negative Sample Judgment Method for Crop Pest Recognition Based on Deep Learning Models},
  author = {Wang, Kaili and Chen, Keyu and Du, Huiyu and Liu, Shuang and Xu, Jingwen and Zhao, Junfang and Chen, Houlin and Liu, Yujun and Liu, Yang},
  year = {2022},
  journal = {Ecological Informatics},
  volume = {69},
  pages = {101620},
  issn = {1574-9541},
  doi = {10.1016/j.ecoinf.2022.101620},
  abstract = {Crop pests are responsible for serious economic loss around the worldwide. Accurate recognition of pests is the key to pest control and is a considerable challenge in farming. Deep learning models have shown great promise in image recognition, drawing the attention of many agricultural experts. However, the lack of pest image datasets and the inexplicability of deep learning models have hindered the development of deep learning models in the field of pest recognition. Our work provides the following four contributions: (1) We constructed a new and more effective dataset, for crop pest recognition, named IP41 comprising 46,567 original images of crop pests in 41 classes. (2) We trained three different deep learning models based on IP41, using transfer learning combined with fine-tuning. The results of the three deep learning models exceeded 80.00\% recognition. (3) A negative sample judgment method was proposed to exclude the uploaded pest-free images of the user. (4) We provided reasonable visual explanations for the most critical areas of the recognition layers by using the gradient-weighted class activation mapping method. This research suggests that the recognition process focuses more on image details than the image as a whole, and that overall difference is ignored to a certain extent. These results will be helpful to future research in the field of agricultural pest recognition},
  keywords = {Crop pests,Deep learning,Image recognition,Negative sample judgment}
}

@article{weiss_plant_2011,
  title = {Plant Detection and Mapping for Agricultural Robots Using a {{3D LIDAR}} Sensor},
  author = {Weiss, Ulrich and Biber, Peter},
  year = {2011},
  journal = {Robotics and Autonomous Systems},
  volume = {59},
  number = {5},
  pages = {265--273},
  issn = {0921-8890},
  doi = {10.1016/j.robot.2011.02.011},
  abstract = {In this article, we discuss the advantages of MEMS based 3D LIDAR sensors over traditional approaches like vision or stereo vision in the domain of agricultural robotics and compare these kinds of sensors with typical 3D sensors used on mobile robots. Further, we present an application for such sensors. This application deals with the detection and segmentation of plants and ground, which is one important prerequisite to perform localization, mapping and navigation for autonomous agricultural robots. We show the discrimination of ground and plants as well as the mapping of the plants. Experiments conducted using the FX6 LIDAR by Nippon Signal were carried out in the simulation environment Gazebo, with artificial maize plants in the laboratory and on a small maize field. Our results show that the tested plants can be reliably detected and segmented from ground, despite the use of the low resolution FX6 sensor. Further, the plants can be localized with high accuracy.},
  keywords = {3D LIDAR sensor,Agricultural robotics,Individual plant detection,Plant mapping}
}

@article{weng_survey_2019,
  title = {{A survey on deep-learning-based plant phenotype research in agriculture}},
  author = {Weng, Yang and Zeng, Rui and Wu, ChenMing and Wang, Meng and Wang, XiuJie and Liu, YongJin},
  year = {2019},
  month = jun,
  journal = {SCIENTIA SINICA Vitae},
  volume = {49},
  number = {6},
  pages = {698--716},
  issn = {1674-7232},
  doi = {10.1360/SSV-2019-0020},
  langid = {chinese},
  file = {C\:\\Users\\HP\\Zotero\\storage\\VV3ZNGNJ\\Weng 等 - 2019 - A survey on deep-learning-based plant phenotype re.pdf}
}

@article{worrall_domain-guided_2021,
  title = {Domain-{{Guided Machine Learning}} for {{Remotely Sensed In-Season Crop Growth Estimation}}},
  author = {Worrall, George and Rangarajan, Anand and Judge, Jasmeet},
  year = {2021},
  journal = {Remote Sensing},
  volume = {13},
  number = {22},
  pages = {4605},
  issn = {2072-4292},
  doi = {10.3390/rs13224605}
}

@article{worrall_domain-guided_2021-1,
  title = {Domain-{{Guided Machine Learning}} for {{Remotely Sensed In-Season Crop Growth Estimation}}},
  author = {Worrall, George and Rangarajan, Anand and Judge, Jasmeet},
  year = {2021},
  month = nov,
  journal = {Remote Sensing},
  volume = {13},
  number = {22},
  pages = {4605},
  issn = {2072-4292},
  doi = {10.3390/rs13224605},
  abstract = {Advanced machine learning techniques have been used in remote sensing (RS) applications such as crop mapping and yield prediction, but remain under-utilized for tracking crop progress. In this study, we demonstrate the use of agronomic knowledge of crop growth drivers in a Long ShortTerm Memory-based, domain-guided neural network (DgNN) for in-season crop progress estimation. The DgNN uses a branched structure and attention to separate independent crop growth drivers and captures their varying importance throughout the growing season. The DgNN is implemented for corn, using RS data in Iowa, U.S., for the period 2003\textendash 2019, with United States Department of Agriculture (USDA) crop progress reports used as ground truth. State-wide DgNN performance shows significant improvement over sequential and dense-only NN structures, and a widely-used Hidden Markov Model method. The DgNN had a 4.0\% higher Nash-Sutcliffe efficiency over all growth stages and 39\% more weeks with highest cosine similarity than the next best NN during test years. The DgNN and Sequential NN were more robust during periods of abnormal crop progress, though estimating the Silking\textendash Grainfill transition was difficult for all methods. Finally, Uniform Manifold Approximation and Projection visualizations of layer activations showed how LSTM-based NNs separate crop growth time-series differently from a dense-only structure. Results from this study exhibit both the viability of NNs in crop growth stage estimation (CGSE) and the benefits of using domain knowledge. The DgNN methodology presented here can be extended to provide near-real time CGSE of other crops.},
  langid = {english},
  file = {C\:\\Users\\HP\\Zotero\\storage\\Y5LI5X7Y\\Worrall 等 - 2021 - Domain-Guided Machine Learning for Remotely Sensed.pdf}
}

@article{xiang_development_2011,
  title = {Development of a Low-Cost Agricultural Remote Sensing System Based on an Autonomous Unmanned Aerial Vehicle ({{UAV}})},
  author = {Xiang, Haitao and Tian, Lei},
  year = {2011},
  journal = {Biosystems Engineering},
  volume = {108},
  number = {2},
  pages = {174--190},
  issn = {1537-5110},
  doi = {10.1016/j.biosystemseng.2010.11.010},
  abstract = {To provide and improved remote sensing a system based on an autonomous UAV was developed. The system was based on an easily transportable helicopter platform weighing less than 14 kg. Equipped with a multi-spectral camera and autonomous system, the UAV system was capable of acquiring multi-spectral images at the desired locations and times. An extended Kalman filter (EKF) based UAV navigation system was designed and implemented using sensor fusion techniques. A ground station was designed to be the interface between a human operator and the UAV to carry out mission planning, flight command activation, and real-time flight monitoring. Based on the navigation data, and the waypoints generated by the ground station, the UAV could be automatically navigated to the desired waypoints and hover around each waypoint to collect field image data. An experiment using the UAV system to monitor turf grass glyphosate application demonstrated the system, which indicated the UAV system provides a flexible and reliable method of sensing agricultural field with high spatial and temporal resolution of image data.}
}

@article{xie_integration_2021,
  title = {Integration of a {{Crop Growth Model}} and {{Deep Learning Methods}} to {{Improve Satellite-Based Yield Estimation}} of {{Winter Wheat}} in {{Henan Province}}, {{China}}},
  author = {Xie, Yi and Huang, Jianxi},
  year = {2021},
  month = oct,
  journal = {Remote Sensing},
  volume = {13},
  number = {21},
  pages = {4372},
  issn = {2072-4292},
  doi = {10.3390/rs13214372},
  abstract = {Timely and accurate regional crop-yield estimates are crucial for guiding agronomic practices and policies to improve food security. In this study, a crop-growth model was integrated with time series of remotely sensed data through deep learning (DL) methods to improve the accuracy of regional wheat-yield estimations in Henan Province, China. Firstly, the time series of moderateresolution imaging spectroradiometer (MODIS) normalized difference vegetation index (NDVI) were input into the long short-term memory network (LSTM) model to identify the wheat-growing region, which was further used to estimate wheat areas at the municipal and county levels. Then, the leaf area index (LAI) and grain-yield time series simulated by the Crop Environment REsource Synthesis for Wheat (CERES-Wheat) model were used to train and evaluate the LSTM, one-dimensional convolutional neural network (1-D CNN) and random forest (RF) models, respectively. Finally, an exponential model of the relationship between the field-measured LAI and MODIS NDVI was applied to obtain the regional LAI, which was input into the trained LSTM, 1-D CNN and RF models to estimate wheat yields within the wheat-growing region. The results showed that the linear correlations between the estimated wheat areas and the statistical areas were significant at both the municipal and county levels. The LSTM model provided more accurate estimates of wheat yields, with higher R2 values and lower root mean square error (RMSE) and mean relative error (MRE) values than the 1-D CNN and RF models. The LSTM model has an inherent advantage in capturing phenological information contained in the time series of the MODIS-derived LAI, which is important for satellite-based crop-yield estimates.},
  langid = {english},
  file = {C\:\\Users\\HP\\Zotero\\storage\\Z6PNRYCB\\Xie 和 Huang - 2021 - Integration of a Crop Growth Model and Deep Learni.pdf}
}

@article{xie_integration_2021-1,
  title = {Integration of a {{Crop Growth Model}} and {{Deep Learning Methods}} to {{Improve Satellite-Based Yield Estimation}} of {{Winter Wheat}} in {{Henan Province}}, {{China}}},
  author = {Xie, Yi and Huang, Jianxi},
  year = {2021},
  journal = {Remote Sensing},
  volume = {13},
  number = {21},
  pages = {4372},
  publisher = {{MDPI}}
}

@misc{xu_multimodal_2022,
  title = {Multimodal {{Learning}} with {{Transformers}}: {{A Survey}}},
  shorttitle = {Multimodal {{Learning}} with {{Transformers}}},
  author = {Xu, Peng and Zhu, Xiatian and Clifton, David A.},
  year = {2022},
  month = jun,
  number = {arXiv:2206.06488},
  eprint = {2206.06488},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Transformer is a promising neural network learner, and has achieved great success in various machine learning tasks. Thanks to the recent prevalence of multimodal applications and big data, Transformer-based multimodal learning has become a hot topic in AI research. This paper presents a comprehensive survey of Transformer techniques oriented at multimodal data. The main contents of this survey include: (1) a background of multimodal learning, Transformer ecosystem, and the multimodal big data era, (2) a theoretical review of Vanilla Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective, (3) a review of multimodal Transformer applications, via two important paradigms, i.e., for multimodal pretraining and for specific multimodal tasks, (4) a summary of the common challenges and designs shared by the multimodal Transformer models and applications, and (5) a discussion of open problems and potential research directions for the community.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\HP\\Zotero\\storage\\55WVX6AA\\Xu 等 - 2022 - Multimodal Learning with Transformers A Survey(翻译结果).docx;C\:\\Users\\HP\\Zotero\\storage\\CZZ4CCED\\Xu 等 - 2022 - Multimodal Learning with Transformers A Survey.pdf}
}

@article{yan_scalable_2021,
  title = {Scalable {{Privacy-preserving Geo-distance Evaluation}} for {{Precision Agriculture IoT Systems}}},
  author = {Yan, Qiben and Lou, Jianzhi and Vuran, Mehmet C. and Irmak, Suat},
  year = {2021},
  month = jul,
  journal = {ACM Transactions on Sensor Networks},
  volume = {17},
  number = {4},
  pages = {1--30},
  issn = {1550-4859, 1550-4867},
  doi = {10.1145/3463575},
  abstract = {Precision agriculture has become a promising paradigm to transform modern agriculture. The recent revolution in big data and Internet-of-Things (IoT) provides unprecedented benefits including optimizing yield, minimizing environmental impact, and reducing cost. However, the mass collection of farm data in IoT applications raises serious concerns about potential privacy leakage that may harm the farmers' welfare. In this work, we propose a novel scalable and private geo-distance evaluation system, called SPRIDE, to allow application servers to provide geographic-based services by computing the distances among sensors and farms privately. The servers determine the distances without learning any additional information about their locations. The key idea of SPRIDE is to perform efficient distance measurement and distance comparison on encrypted locations over a sphere by leveraging a homomorphic cryptosystem. To serve a large user base, we further propose SPRIDE+ with novel and practical performance enhancements based on pre-computation of cryptographic elements. Through extensive experiments using real-world datasets, we show SPRIDE+ achieves private distance evaluation on a large network of farms, attaining 3+ times runtime performance improvement over existing techniques. We further show SPRIDE+ can run on resource-constrained mobile devices, which offers a practical solution for privacy-preserving precision agriculture IoT applications.},
  langid = {english},
  file = {C\:\\Users\\HP\\Zotero\\storage\\6PBFR8V4\\Yan 等 - 2021 - Scalable Privacy-preserving Geo-distance Evaluatio.pdf}
}

@article{yang_deep_2019,
  title = {Deep Convolutional Neural Networks for Rice Grain Yield Estimation at the Ripening Stage Using {{UAV-based}} Remotely Sensed Images},
  author = {Yang, Qi and Shi, Liangsheng and Han, Jinye and Zha, Yuanyuan and Zhu, Penghui},
  year = {2019},
  journal = {Field Crops Research},
  volume = {235},
  pages = {142--153},
  publisher = {{Elsevier}}
}

@article{yang_deep_2019-1,
  title = {Deep Convolutional Neural Networks for Rice Grain Yield Estimation at the Ripening Stage Using {{UAV-based}} Remotely Sensed Images},
  author = {Yang, Qi and Shi, Liangsheng and Han, Jinye and Zha, Yuanyuan and Zhu, Penghui},
  year = {2019},
  journal = {Field Crops Research},
  volume = {235},
  pages = {142--153},
  issn = {0378-4290},
  doi = {10.1016/j.fcr.2019.02.022},
  abstract = {Forecasting rice grain yield prior to harvest is essential for crop management, food security evaluation, food trade, and policy-making. Many successful applications have been made in crop yield estimation using remotely sensed products, such as vegetation index (VI) from multispectral imagery. However, VI-based approaches are only suitable for estimating rice grain yield at the middle stage of growth but have limited capability at the ripening stage. In this study, an efficient convolutional neural network (CNN) architecture was proposed to learn the important features related to rice grain yield from low-altitude remotely sensed imagery. In one major region for rice cultivation of Southern China, a 160-hectare site with over 800 management units was chosen to investigate the ability of CNN in rice grain yield estimation. The datasets of RGB and multispectral images were obtained by a fixed-wing, unmanned aerial vehicle (UAV), which was mounted with a digital camera and multispectral sensors. The network was trained with different datasets and compared against the traditional vegetation index-based method. In addition, the temporal and spatial generality of the trained network was investigated. The results showed that the CNNs trained by RGB and multispectral datasets perform much better than VIs-based regression model for rice grain yield estimation at the ripening stage. The RGB imagery of very high spatial resolution contains important spatial features with respect to grain yield distribution, which can be learned by deep CNN. The results highlight the promising potential of deep convolutional neural networks for rice grain yield estimation with excellent spatial and temporal generality, and a wider time window of yield forecasting.},
  keywords = {CNN,Deep learning,Rice crop,UAV,Yield estimation}
}

@article{yazdavar_multimodal_2020,
  title = {Multimodal Mental Health Analysis in Social Media},
  author = {Yazdavar, Amir Hossein and Mahdavinejad, Mohammad Saeid and Bajaj, Goonmeet and Romine, William and Sheth, Amit and Monadjemi, Amir Hassan and Thirunarayan, Krishnaprasad and Meddar, John M and Myers, Annie and Pathak, Jyotishman and others},
  year = {2020},
  journal = {Plos one},
  volume = {15},
  number = {4},
  pages = {e0226248},
  publisher = {{Public Library of Science San Francisco, CA USA}}
}

@article{yazdavar_multimodal_2020-1,
  title = {Multimodal Mental Health Analysis in Social Media},
  author = {Yazdavar, Amir Hossein and Mahdavinejad, Mohammad Saeid and Bajaj, Goonmeet and Romine, William and Sheth, Amit and Monadjemi, Amir Hassan and Thirunarayan, Krishnaprasad and Meddar, John M and Myers, Annie and Pathak, Jyotishman and others},
  year = {2020},
  journal = {Plos one},
  volume = {15},
  number = {4},
  pages = {e0226248},
  publisher = {{Public Library of Science San Francisco, CA USA}}
}

@article{yu_automatic_2013,
  title = {Automatic Image-Based Detection Technology for Two Critical Growth Stages of Maize: {{Emergence}} and Three-Leaf Stage},
  author = {Yu, Z. and Cao, Z. and Wu, X. and Bai, X. and Qin, Y. and Zhuo, W. and Xiao, Y. and Zhang, X. and Xue, H.},
  year = {2013},
  journal = {Agricultural \& Forest Meteorology},
  volume = {174--175},
  number = {Complete},
  pages = {65--84}
}

@article{yuan_deep_2020,
  title = {Deep Learning in Environmental Remote Sensing: {{Achievements}} and Challenges},
  author = {Yuan, Qiangqiang and Shen, Huanfeng and Li, Tongwen and Li, Zhiwei and Li, Shuwen and Jiang, Yun and Xu, Hongzhang and Tan, Weiwei and Yang, Qianqian and Wang, Jiwen and others},
  year = {2020},
  journal = {Remote Sensing of Environment},
  volume = {241},
  pages = {111716},
  publisher = {{Elsevier}}
}

@article{yuan_indicators_2016,
  title = {Indicators for Diagnosing Nitrogen Status of Rice Based on Chlorophyll Meter Readings},
  author = {Yuan, Zhaofeng and {Ata-Ul-Karim}, Syed Tahir and Cao, Qiang and Lu, Zhenzhou and Cao, Weixing and Zhu, Yan and Liu, Xiaojun},
  year = {2016},
  journal = {Field Crops Research},
  volume = {185},
  pages = {12--20},
  publisher = {{Elsevier}}
}

@article{yue_estimate_2019,
  title = {Estimate of Winter-Wheat above-Ground Biomass Based on {{UAV}} Ultrahigh-Ground-Resolution Image Textures and Vegetation Indices},
  author = {Yue, Jibo and Yang, Guijun and Tian, Qingjiu and Feng, Haikuan and Xu, Kaijian and Zhou, Chengquan},
  year = {2019},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {150},
  pages = {226--244},
  issn = {0924-2716},
  doi = {10.1016/j.isprsjprs.2019.02.022},
  abstract = {When dealing with multiple growth stages, estimates of above-ground biomass (AGB) based on optical vegetation indices (VIs) are difficult for two reasons: (i) optical VIs saturate at medium-to-high canopy cover, and (ii) organs that grow vertically (e.g., biomass of reproductive organs and stems) are difficult to detect by canopy spectral VIs. Although several significant improvements have been made for estimating AGB by using narrow-band hyperspectral VIs, synthetic aperture radar, laser intensity direction and ranging, the crop surface model technique, and combinations thereof, applications of these new techniques have been limited by cost, availability, data-processing difficulties, and high dimensionality. The present study thus evaluates the use of ultrahigh-ground-resolution image textures, VIs, and combinations thereof to make multiple temporal estimates and maps of AGB covering three winter-wheat growth stages. The selected gray-tone spatial-dependence matrix-based image textures (e.g., variance, entropy, data range, homogeneity, second moment, dissimilarity, contrast, correlation) are calculated from 1-, 2-, 5-, 10-, 15-, 20-, 25-, and 30-cm-ground-resolution images acquired by using an inexpensive RGB sensor mounted on an unmanned aerial vehicle (UAV). Optical-VI data were obtained by using a ground spectrometer to analyze UAV-acquired RGB images. The accuracy of AGB estimates based on optical VIs varies, with validation R2: 0.59\textendash 0.78, root mean square error (RMSE): 1.22\textendash 1.59 t/ha, and mean absolute error (MAE): 1.03\textendash 1.27 t/ha. The most accurate AGB estimate was obtained by combining image textures and VIs, which gave R2: 0.89, MAE: 0.67 t/ha, and RMSE: 0.82 t/ha. The results show that (i) the eight selected textures from ultrahigh-ground-resolution images were significantly related to AGB, (ii) the combined use of image textures from 1- to 30-cm-ground-resolution images and VIs can improve the accuracy of AGB estimates as compared with using only optical VIs or image textures alone; and (iii) high AGB values from winter-wheat reproductive growth stages can be accurately estimated by using this method; (iv) high estimates of winter-wheat AGB (8\textendash 14 t/ha) using the proposed combined method (DIS1, SE30, B460, B560, B670, EVI2 using MSR) show a 22.63\% (nRMSE) improvement compared with using only spectral VIs (LCI, NDVI using MSR), and a 21.24\% (nRMSE) improvement compared with using only image textures (COR1, DIS1, SE30, EN30 using MSR). Thus, the combined use of image textures and VIs can help improve estimates of AGB under conditions of high canopy coverage.},
  keywords = {Gray-tone spatial-dependence matrix,Image textures,Reproductive growth stages,Ultrahigh ground-resolution image,Unmanned aerial vehicle,Vegetation indices}
}

@article{yuhas_integration_1989,
  title = {Integration of Acoustic and Visual Speech Signals Using Neural Networks},
  author = {Yuhas, Ben P and Goldstein, Moise H and Sejnowski, Terrence J},
  year = {1989},
  journal = {IEEE Communications Magazine},
  volume = {27},
  number = {11},
  pages = {65--71},
  publisher = {{IEEE}}
}

@article{zhang_using_2019,
  title = {Using a {{Portable Active Sensor}} to {{Monitor Growth Parameters}} and {{Predict Grain Yield}} of {{Winter Wheat}}},
  author = {Zhang, Jiayi and Liu, Xia and Liang, Yan and Cao, Qiang and Tian, Yongchao and Zhu, Yan and Cao, Weixing and Liu, Xiaojun},
  year = {2019},
  month = mar,
  journal = {Sensors},
  volume = {19},
  number = {5},
  pages = {1108},
  issn = {1424-8220},
  doi = {10.3390/s19051108},
  abstract = {Rapid and effective acquisition of crop growth information is a crucial step of precision agriculture for making in-season management decisions. Active canopy sensor GreenSeeker (Trimble Navigation Limited, Sunnyvale, CA, USA) is a portable device commonly used for non-destructively obtaining crop growth information. This study intended to expand the applicability of GreenSeeker in monitoring growth status and predicting grain yield of winter wheat (Triticum aestivum L.). Four field experiments with multiple wheat cultivars and N treatments were conducted during 2013\textendash 2015 for obtaining canopy normalized difference vegetation index (NDVI) and ratio vegetation index (RVI) synchronized with four agronomic parameters: leaf area index (LAI), leaf dry matter (LDM), leaf nitrogen concentration (LNC), and leaf nitrogen accumulation (LNA). Duration models based on NDVI and RVI were developed to monitor these parameters, which indicated that NDVI and RVI explained 80\%, 68\textendash 70\%, 10\textendash 12\%, and 67\textendash 73\% of the variability in LAI, LDM, LNC and LNA, respectively. According to the validation results, the relative root mean square error (RRMSE) were all {$<$}0.24 and the relative error (RE) were all {$<$}23\%. Considering the variation among different wheat cultivars, the newly normalized vegetation indices rNDVI (NDVI vs. the NDVI for the highest N rate) and rRVI (RVI vs. the RVI for the highest N rate) were calculated to predict the relative grain yield (RY, the yield vs. the yield for the highest N rate). rNDVI and rRVI explained 77\textendash 85\% of the variability in RY, the RRMSEs were both {$<$}0.13 and the REs were both {$<$}6.3\%. The result demonstrates the feasibility of monitoring growth parameters and predicting grain yield of winter wheat with portable GreenSeeker sensor.},
  langid = {english},
  file = {C\:\\Users\\HP\\Zotero\\storage\\MYWUMH3V\\Zhang 等 - 2019 - Using a Portable Active Sensor to Monitor Growth P(翻译结果).docx;C\:\\Users\\HP\\Zotero\\storage\\ZNBKTS8T\\Zhang 等 - 2019 - Using a Portable Active Sensor to Monitor Growth P.pdf}
}

@misc{zhao_battle_2021,
  title = {A {{Battle}} of {{Network Structures}}: {{An Empirical Study}} of {{CNN}}, {{Transformer}}, and {{MLP}}},
  shorttitle = {A {{Battle}} of {{Network Structures}}},
  author = {Zhao, Yucheng and Wang, Guangting and Tang, Chuanxin and Luo, Chong and Zeng, Wenjun and Zha, Zheng-Jun},
  year = {2021},
  month = nov,
  number = {arXiv:2108.13002},
  eprint = {2108.13002},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Convolutional neural networks (CNN) are the dominant deep neural network (DNN) architecture for computer vision. Recently, Transformer and multi-layer perceptron (MLP)-based models, such as Vision Transformer and MLP-Mixer, started to lead new trends as they showed promising results in the ImageNet classification task. In this paper, we conduct empirical studies on these DNN structures and try to understand their respective pros and cons. To ensure a fair comparison, we first develop a unified framework called SPACH which adopts separate modules for spatial and channel processing. Our experiments under the SPACH framework reveal that all structures can achieve competitive performance at a moderate scale. However, they demonstrate distinctive behaviors when the network size scales up. Based on our findings, we propose two hybrid models using convolution and Transformer modules. The resulting Hybrid-MS-S+ model achieves 83.9\% top-1 accuracy with 63M parameters and 12.3G FLOPS. It is already on par with the SOTA models with sophisticated designs. The code and models are publicly available at https://github.com/microsoft/SPACH .},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\HP\\Zotero\\storage\\4GPNR77H\\Zhao 等 - 2021 - A Battle of Network Structures An Empirical Study.pdf;C\:\\Users\\HP\\Zotero\\storage\\N76ZIB7X\\Zhao 等 - 2021 - A Battle of Network Structures An Empirical Study(翻译结果).docx}
}

@article{zhong_deep_2019,
  title = {Deep Learning Based Multi-Temporal Crop Classification},
  author = {Zhong, Liheng and Hu, Lina and Zhou, Hang},
  year = {2019},
  journal = {Remote sensing of environment},
  volume = {221},
  pages = {430--443},
  publisher = {{Elsevier}}
}

@article{zhou_predicting_2017,
  title = {Predicting Grain Yield in Rice Using Multi-Temporal Vegetation Indices from {{UAV-based}} Multispectral and Digital Imagery},
  author = {Zhou, X. and Zheng, H. B. and Xu, X. Q. and He, J. Y. and Ge, X. K. and Yao, X. and Cheng, T. and Zhu, Y. and Cao, W. X. and Tian, Y. C.},
  year = {2017},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {130},
  pages = {246--255},
  issn = {0924-2716},
  doi = {10.1016/j.isprsjprs.2017.05.003},
  abstract = {Timely and non-destructive assessment of crop yield is an essential part of agricultural remote sensing (RS). The development of unmanned aerial vehicles (UAVs) has provided a novel approach for RS, and makes it possible to acquire high spatio-temporal resolution imagery on a regional scale. In this study, the rice grain yield was predicted with single stage vegetation indices (VIs) and multi-temporal VIs derived from the multispectral (MS) and digital images. The results showed that the booting stage was identified as the optimal stage for grain yield prediction with VIs at a single stage for both digital image and MS image. And corresponding optimal color index was VARI with R2 value of 0.71 (Log relationship). While the optimal vegetation index NDVI[800,720] based on MS images showed a linear relationship with the grain yield and gained a higher R2 value (0.75) than color index did. The multi-temporal VIs showed a higher correlation with grain yield than the single stage VIs did. And the VIs at two random growth stage with the multiple linear regression function [MLR(VI)] performed best. The highest correlation coefficient were 0.76 with MLR(NDVI[800,720]) at the booting and heading stages (for the MS image) and 0.73 with MLR(VARI) at the jointing and booting stages (for the digital image). In addition, the VIs that showed a high correlation with LAI performed well for yield prediction, and the VIs composed of red edge band (720nm) and near infrared band (800nm) were found to be more effective in predicting yield and LAI at high level. In conclusion, this study has demonstrated that both MS and digital sensors mounted on the UAV are reliable platforms for rice growth and grain yield estimation, and determined the best period and optimal VIs for rice grain yield prediction.},
  keywords = {Digital image,Grain yield prediction,Multispectral image,Rice,UAVs}
}

@article{zhu_-field_2016,
  title = {In-Field Automatic Observation of Wheat Heading Stage Using Computer Vision},
  author = {Zhu, Yanjun and Cao, Zhiguo and Lu, Hao and Li, Yanan and Xiao, Yang},
  year = {2016},
  journal = {Biosystems Engineering},
  volume = {143},
  pages = {28--41},
  issn = {1537-5110},
  doi = {10.1016/j.biosystemseng.2015.12.015},
  abstract = {Growth stage information is an important factor for precision agriculture. It provides accurate evidence for agricultural management as well as early evaluation of yield. However, the observation of critical growth stages mainly relies on manual labour at present. This has some limitations because it is time-consuming, discontinuous and non-objective. Computer vision technology can help to alleviate these difficulties when monitoring growth status. This paper describes a novel automatic observation system for wheat heading stage based on computer vision. Images compliant with statistical requirements are taken in natural conditions where illumination changes frequently. Wheat plants with low spatial resolution overlap substantially, which increases observational difficulties. To adapt to the complex environment, a two-step coarse-to-fine wheat ear detection mechanism is proposed. In the coarse-detection step, machine learning technology is used to emphasise the candidate ear regions. In the fine-detection step, non-ear areas are eliminated through higher-level features. For that purpose, scale-invariant feature transform (SIFT) is densely extracted as the low-level visual descriptor, then Fisher vector (FV) encoding is employed to generate the mid-level representation. Based on three consecutive year's data of seven image sequences, a series of experiments are conducted to demonstrate the effectiveness and robustness of our proposition. Experimental results show that the proposed method significantly outperforms other existing methods with an average value of absolute error of 1.14 days on the test dataset. The results indicate that automatic observation is quite acceptable compared to manual observations.},
  keywords = {Automatic observation,Computer vision,FV,Heading stage,SIFT}
}
