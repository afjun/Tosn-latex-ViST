\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{Abstract}{1}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{book,BRISSON2003309,boogaard1998wofost}
\citation{lecun_deep_2015,2020Deep}
\citation{wang_new_2022,yue_estimate_2019,turkoglu_crop_2021}
\citation{trnka_effect_2007,islam_deep_2018,adisa_application_2019,liu_neural_2001,matsumura_maize_2015}
\citation{__1999}
\citation{carlson_relation_1997}
\citation{guo2019deep,xiao2020multimodal}
\citation{__2016}
\citation{Xu_field_2008,Tang_field_2011}
\citation{Chen_field_2022}
\citation{johnson_crop_2016,zhong_deep_2019,yang_deep_2019}
\citation{weiss_plant_2011,tao_estimation_2020,zhou_predicting_2017,maimaitijiang_unmanned_2017,wan_grain_2020}
\citation{fortin_site-specific_2011,campbell_effect_1988,ehret_neural_2011}
\citation{dahikar_agricultural_2014}
\citation{oneal_neural_2002}
\citation{morimoto_dynamic_2007}
\citation{drummond_application_1998}
\citation{kitchen_soil_2003}
\citation{padilla_proximal_2018}
\citation{sengupta_review_2020,8269806,liang_foundations_2023}
\citation{yuhas_integration_1989}
\citation{snoek_multimodal_2005}
\citation{chen_heu_2021}
\citation{lei_video_2021}
\citation{long_improving_2021}
\citation{souza_online_2021}
\citation{yazdavar_multimodal_2020}
\citation{huang_what_2021}
\citation{dang2021autumn}
\citation{chu_end--end_2020}
\citation{maimaitijiang_soybean_2020}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Single-modality approaches for crop growth prediction}{3}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Multimodal learning for crop growth prediction}{3}{subsection.2.2}\protected@file@percent }
\citation{zhao_battle_2021}
\citation{clevers_remote_2013}
\citation{prakash_multi-modal_2021}
\citation{nagrani_attention_nodate}
\citation{vaswani_attention_2017}
\citation{8269806}
\citation{vaswani_attention_2017}
\citation{maimaitijiang_soybean_2020}
\citation{dosovitskiy_image_2021}
\@writefile{toc}{\contentsline {section}{\numberline {3}Vision-and-Sensor Transformer model}{4}{section.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The framework of ViST for growth prediction. Solid lines represent forward and dashed lines represent loop. The input of the ViST is sensor and image data. They are processed independently at MLP and LPFP modules, respectively. The features from the two modules are input to one Transformer Encoder for feature fusion. The encoder output is given to the Concat module with the output for Multiple Modalities (MM). At the same time, the features are sent separately to the other two Transformer encoders for self-attention mechanics. The outputs of these two transformer encoders are Single Modality with Image(SMI) and Single Modal with Sensor(SMS). The results of MM, SMI, and SMS are then input into the Pooler module to reduce the dimension of the features. Finally, the features are input to the linear layer module to output the leaf area index (LAI) value (in the range of [0,1]).\relax }}{5}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{model_structure}{{1}{5}{The framework of ViST for growth prediction. Solid lines represent forward and dashed lines represent loop. The input of the ViST is sensor and image data. They are processed independently at MLP and LPFP modules, respectively. The features from the two modules are input to one Transformer Encoder for feature fusion. The encoder output is given to the Concat module with the output for Multiple Modalities (MM). At the same time, the features are sent separately to the other two Transformer encoders for self-attention mechanics. The outputs of these two transformer encoders are Single Modality with Image(SMI) and Single Modal with Sensor(SMS). The results of MM, SMI, and SMS are then input into the Pooler module to reduce the dimension of the features. Finally, the features are input to the linear layer module to output the leaf area index (LAI) value (in the range of [0,1]).\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}MLP Module}{5}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Sensor input model structure. The MLP module is a multilayer perceptron. It has 19 neurons at the input layer and 768 neurons at the output layer. The two hidden layers contain 32 and 64 neurons, respectively. We found the performance using 2-layer MLP and 3-layer MLP doesn’t have a significant difference, thus we used the simpler 2-layer MLP.\relax }}{6}{figure.caption.3}\protected@file@percent }
\newlabel{mlp_module}{{2}{6}{Sensor input model structure. The MLP module is a multilayer perceptron. It has 19 neurons at the input layer and 768 neurons at the output layer. The two hidden layers contain 32 and 64 neurons, respectively. We found the performance using 2-layer MLP and 3-layer MLP doesn’t have a significant difference, thus we used the simpler 2-layer MLP.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Linear Projection Flattened Patches module}{6}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Linear Projection Flattened Patches module. Linear Projection Flattened Patches divide the image into several equal small images and align the image feature map with the sensor features through matrix transformation.\relax }}{6}{figure.caption.4}\protected@file@percent }
\newlabel{linear_projection}{{3}{6}{Linear Projection Flattened Patches module. Linear Projection Flattened Patches divide the image into several equal small images and align the image feature map with the sensor features through matrix transformation.\relax }{figure.caption.4}{}}
\citation{dosovitskiy_image_2021}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Transformer Encoder for fusion}{7}{subsection.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The structure of the Transformer Encoder for Fusion (TEF) model consists of alternating layers of Multi-Head Cross Attention (MHCA), multiple layer perceptron (MLP) and Layer Normalization (LN) modules. MHCA is the central module responsible for fusing the image and sensor features. The MLP is adopted from ViT, and the LN module is responsible for performing Layer Normalization operations.\relax }}{7}{figure.caption.5}\protected@file@percent }
\newlabel{model_structure_TEF}{{4}{7}{The structure of the Transformer Encoder for Fusion (TEF) model consists of alternating layers of Multi-Head Cross Attention (MHCA), multiple layer perceptron (MLP) and Layer Normalization (LN) modules. MHCA is the central module responsible for fusing the image and sensor features. The MLP is adopted from ViT, and the LN module is responsible for performing Layer Normalization operations.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The Multi-Head Cross Attention Module (MHCA) structure serves as a central component of the Transformer Encoder for Fusion. It operates by taking in both the image feature map (\begin  {math} I_{in} \end  {math}) and the sensor feature map (\begin  {math} S_{in} \end  {math}) as inputs. The module employs cross-attention mechanisms to investigate the interrelationships and associations among each feature map and between the two feature maps. Through this approach, the MHCA module achieves an improved ability to identify meaningful features from both modalities, generating an image feature map with attention (\begin  {math} I_{attn} \end  {math}) and a sensor feature map with attention (\begin  {math} S_{attn} \end  {math}) as outputs.\relax }}{8}{figure.caption.6}\protected@file@percent }
\newlabel{cross_attention}{{5}{8}{The Multi-Head Cross Attention Module (MHCA) structure serves as a central component of the Transformer Encoder for Fusion. It operates by taking in both the image feature map (\begin {math} I_{in} \end {math}) and the sensor feature map (\begin {math} S_{in} \end {math}) as inputs. The module employs cross-attention mechanisms to investigate the interrelationships and associations among each feature map and between the two feature maps. Through this approach, the MHCA module achieves an improved ability to identify meaningful features from both modalities, generating an image feature map with attention (\begin {math} I_{attn} \end {math}) and a sensor feature map with attention (\begin {math} S_{attn} \end {math}) as outputs.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Loss function}{9}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments and Results}{9}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Data collection}{9}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The data were collected at Farm 290, located in Suibin County, Hegang City, Heilongjiang Province, China.\relax }}{10}{figure.caption.7}\protected@file@percent }
\newlabel{farm_location}{{6}{10}{The data were collected at Farm 290, located in Suibin County, Hegang City, Heilongjiang Province, China.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Data acquisition equipment. The image and sensor data collection device is shown on the left, the leaf area index data collection device is shown in the middle, and the image data is shown on the right.\relax }}{10}{figure.caption.8}\protected@file@percent }
\newlabel{data_collection_equipment}{{7}{10}{Data acquisition equipment. The image and sensor data collection device is shown on the left, the leaf area index data collection device is shown in the middle, and the image data is shown on the right.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Data preprocessing}{10}{subsection.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Crop image data.\relax }}{11}{figure.caption.9}\protected@file@percent }
\newlabel{crop}{{8}{11}{Crop image data.\relax }{figure.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Sensor data. The table lists the data items captured by the sensors and their corresponding units.\relax }}{11}{table.caption.10}\protected@file@percent }
\newlabel{tab:sensor_and_data_items}{{1}{11}{Sensor data. The table lists the data items captured by the sensors and their corresponding units.\relax }{table.caption.10}{}}
\citation{fritsch1980monotone}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Image standardized data. The table presents the mean and standard deviation values for the RGB channels of the images of three crops, namely rice, soybean, and corn. These values were obtained through the normalization process described in the previous section.\relax }}{12}{table.caption.11}\protected@file@percent }
\newlabel{tab:image_standardized_data}{{2}{12}{Image standardized data. The table presents the mean and standard deviation values for the RGB channels of the images of three crops, namely rice, soybean, and corn. These values were obtained through the normalization process described in the previous section.\relax }{table.caption.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Sensor input metrics preprocessing. The table includes the indicators for sensor data processing, as well as their maximum and minimum values. \relax }}{13}{table.caption.12}\protected@file@percent }
\newlabel{tab:sensor_input_metrics_preprocessing}{{3}{13}{Sensor input metrics preprocessing. The table includes the indicators for sensor data processing, as well as their maximum and minimum values. \relax }{table.caption.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces The quantity of experimental data.\relax }}{13}{table.caption.14}\protected@file@percent }
\newlabel{dataset_size}{{4}{13}{The quantity of experimental data.\relax }{table.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Evaluation Metrics}{13}{subsection.4.3}\protected@file@percent }
\newlabel{mae}{{12}{13}{Evaluation Metrics}{equation.4.12}{}}
\newlabel{1}{{9a}{14}{Before and after rice LAI interpolation \relax }{figure.caption.13}{}}
\newlabel{sub@1}{{a}{14}{Before and after rice LAI interpolation \relax }{figure.caption.13}{}}
\newlabel{2}{{9b}{14}{Before and after soybean LAI interpolation \relax }{figure.caption.13}{}}
\newlabel{sub@2}{{b}{14}{Before and after soybean LAI interpolation \relax }{figure.caption.13}{}}
\newlabel{3}{{9c}{14}{Before and after maize LAI interpolation \relax }{figure.caption.13}{}}
\newlabel{sub@3}{{c}{14}{Before and after maize LAI interpolation \relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces LAI data preprocessing. The figure shows the leaf area index data before and after interpolation. The horizontal axis represents time, and the vertical axis represents the leaf area index.\relax }}{14}{figure.caption.13}\protected@file@percent }
\newlabel{lai}{{9}{14}{LAI data preprocessing. The figure shows the leaf area index data before and after interpolation. The horizontal axis represents time, and the vertical axis represents the leaf area index.\relax }{figure.caption.13}{}}
\newlabel{mse}{{13}{14}{Evaluation Metrics}{equation.4.13}{}}
\citation{9672157}
\citation{9864182}
\citation{8999620}
\citation{maimaitijiang_soybean_2020}
\newlabel{mape}{{14}{15}{Evaluation Metrics}{equation.4.14}{}}
\newlabel{smape}{{15}{15}{Evaluation Metrics}{equation.4.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Experiments and results}{15}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Experiments and results for a single crop}{15}{subsubsection.4.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Main hyperparameters of the model.\relax }}{16}{table.caption.15}\protected@file@percent }
\newlabel{tab:hyperparameters}{{5}{16}{Main hyperparameters of the model.\relax }{table.caption.15}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces ViST model test results for different input modes.\relax }}{16}{table.caption.16}\protected@file@percent }
\newlabel{tab:vist_model_test_results}{{6}{16}{ViST model test results for different input modes.\relax }{table.caption.16}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces The comparative experimental test results of rice. The "Mean" and "Std" represent the mean value and standard deviation, respectively, of the results obtained from five experimental groups.\relax }}{17}{table.caption.17}\protected@file@percent }
\newlabel{rice_results}{{7}{17}{The comparative experimental test results of rice. The "Mean" and "Std" represent the mean value and standard deviation, respectively, of the results obtained from five experimental groups.\relax }{table.caption.17}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces The comparative experimental test results of soybean. The "Mean" and "Std" represent the mean value and standard deviation, respectively, of the results obtained from five experimental groups.\relax }}{18}{table.caption.18}\protected@file@percent }
\newlabel{soybean_results}{{8}{18}{The comparative experimental test results of soybean. The "Mean" and "Std" represent the mean value and standard deviation, respectively, of the results obtained from five experimental groups.\relax }{table.caption.18}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces The comparative experimental test results of maize. The "Mean" and "Std" represent the mean value and standard deviation, respectively, of the results obtained from five experimental groups.\relax }}{18}{table.caption.19}\protected@file@percent }
\newlabel{tab:maize_results}{{9}{18}{The comparative experimental test results of maize. The "Mean" and "Std" represent the mean value and standard deviation, respectively, of the results obtained from five experimental groups.\relax }{table.caption.19}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces The comparative experimental test results for hybrid. The "Mean" and "Std" represent the mean value and standard deviation, respectively, of the results obtained from five experimental groups.\relax }}{19}{table.caption.20}\protected@file@percent }
\newlabel{tab:hybrid_results}{{10}{19}{The comparative experimental test results for hybrid. The "Mean" and "Std" represent the mean value and standard deviation, respectively, of the results obtained from five experimental groups.\relax }{table.caption.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Experiments and results for combined data training}{19}{table.caption.20}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces Comparison Results of ViST model with different crops as input.\relax }}{19}{table.caption.21}\protected@file@percent }
\newlabel{tab:ubiquitous}{{11}{19}{Comparison Results of ViST model with different crops as input.\relax }{table.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Speed test results of the models. To compare the inference speed of models, each model performed 1000 inference runs, and the average result was taken as the final outcome. The horizontal axis represents the model name, and the vertical axis represents the average time for each inference run, in milliseconds. Each experimental result has an error rate of \begin  {math} \pm \end  {math}5\%.\relax }}{20}{figure.caption.22}\protected@file@percent }
\newlabel{speed_test}{{10}{20}{Speed test results of the models. To compare the inference speed of models, each model performed 1000 inference runs, and the average result was taken as the final outcome. The horizontal axis represents the model name, and the vertical axis represents the average time for each inference run, in milliseconds. Each experimental result has an error rate of \begin {math} \pm \end {math}5\%.\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.3}Experiments and results for speed test}{20}{subsubsection.4.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion and future directions}{20}{section.5}\protected@file@percent }
\bibstyle{ACM-Reference-Format}
\bibdata{sample-base}
\bibcite{book}{{1}{2014}{{Gaydon and Roth}}{{}}}
\bibcite{BRISSON2003309}{{2}{2003}{{Brisson et~al\mbox  {.}}}{{}}}
\bibcite{boogaard1998wofost}{{3}{1998}{{Boogaard et~al\mbox  {.}}}{{}}}
\bibcite{lecun_deep_2015}{{4}{2015}{{LeCun et~al\mbox  {.}}}{{}}}
\bibcite{2020Deep}{{5}{2020}{{Hu et~al\mbox  {.}}}{{}}}
\bibcite{wang_new_2022}{{6}{2022}{{Wang et~al\mbox  {.}}}{{}}}
\bibcite{yue_estimate_2019}{{7}{2019}{{Yue et~al\mbox  {.}}}{{}}}
\bibcite{turkoglu_crop_2021}{{8}{2021}{{Turkoglu et~al\mbox  {.}}}{{}}}
\bibcite{trnka_effect_2007}{{9}{2007}{{Trnka et~al\mbox  {.}}}{{}}}
\bibcite{islam_deep_2018}{{10}{2018}{{Islam et~al\mbox  {.}}}{{}}}
\bibcite{adisa_application_2019}{{11}{2019}{{Adisa et~al\mbox  {.}}}{{}}}
\bibcite{liu_neural_2001}{{12}{2001}{{Liu et~al\mbox  {.}}}{{}}}
\bibcite{matsumura_maize_2015}{{13}{2015}{{Matsumura et~al\mbox  {.}}}{{}}}
\bibcite{__1999}{{14}{1999}{{Bangjie~Yang}}{{}}}
\bibcite{carlson_relation_1997}{{15}{1997}{{Carlson and Ripley}}{{}}}
\bibcite{guo2019deep}{{16}{2019}{{Guo et~al\mbox  {.}}}{{}}}
\bibcite{xiao2020multimodal}{{17}{2022}{{Xiao et~al\mbox  {.}}}{{}}}
\@writefile{toc}{\contentsline {section}{Acknowledgments}{21}{section*.24}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{References}{21}{section*.26}\protected@file@percent }
\bibcite{__2016}{{18}{2016}{{Chen et~al\mbox  {.}}}{{}}}
\bibcite{Xu_field_2008}{{19}{2008}{{Xu et~al\mbox  {.}}}{{}}}
\bibcite{Tang_field_2011}{{20}{2011}{{Tang et~al\mbox  {.}}}{{}}}
\bibcite{Chen_field_2022}{{21}{2022}{{Chen et~al\mbox  {.}}}{{}}}
\bibcite{johnson_crop_2016}{{22}{2016}{{Johnson et~al\mbox  {.}}}{{}}}
\bibcite{zhong_deep_2019}{{23}{2019}{{Zhong et~al\mbox  {.}}}{{}}}
\bibcite{yang_deep_2019}{{24}{2019}{{Yang et~al\mbox  {.}}}{{}}}
\bibcite{weiss_plant_2011}{{25}{2011}{{Weiss and Biber}}{{}}}
\bibcite{tao_estimation_2020}{{26}{2020}{{Tao et~al\mbox  {.}}}{{}}}
\bibcite{zhou_predicting_2017}{{27}{2017}{{Zhou et~al\mbox  {.}}}{{}}}
\bibcite{maimaitijiang_unmanned_2017}{{28}{2017}{{Maimaitijiang et~al\mbox  {.}}}{{}}}
\bibcite{wan_grain_2020}{{29}{2020}{{Wan et~al\mbox  {.}}}{{}}}
\bibcite{fortin_site-specific_2011}{{30}{2011}{{Fortin et~al\mbox  {.}}}{{}}}
\bibcite{campbell_effect_1988}{{31}{1988}{{Campbell et~al\mbox  {.}}}{{}}}
\bibcite{ehret_neural_2011}{{32}{2011}{{Ehret et~al\mbox  {.}}}{{}}}
\bibcite{dahikar_agricultural_2014}{{33}{2014}{{Dahikar and Rode}}{{}}}
\bibcite{oneal_neural_2002}{{34}{2002}{{O'Neal et~al\mbox  {.}}}{{}}}
\bibcite{morimoto_dynamic_2007}{{35}{2007}{{Morimoto et~al\mbox  {.}}}{{}}}
\bibcite{drummond_application_1998}{{36}{1998}{{Drummond et~al\mbox  {.}}}{{}}}
\bibcite{kitchen_soil_2003}{{37}{2003}{{Kitchen et~al\mbox  {.}}}{{}}}
\bibcite{padilla_proximal_2018}{{38}{2018}{{Padilla et~al\mbox  {.}}}{{}}}
\bibcite{sengupta_review_2020}{{39}{2020}{{Sengupta et~al\mbox  {.}}}{{}}}
\bibcite{8269806}{{40}{2019}{{Baltru{\v s}aitis et~al\mbox  {.}}}{{}}}
\bibcite{liang_foundations_2023}{{41}{2023}{{Liang et~al\mbox  {.}}}{{}}}
\bibcite{yuhas_integration_1989}{{42}{1989}{{Yuhas et~al\mbox  {.}}}{{}}}
\bibcite{snoek_multimodal_2005}{{43}{2005}{{Snoek and Worring}}{{}}}
\bibcite{chen_heu_2021}{{44}{2021}{{Chen et~al\mbox  {.}}}{{}}}
\bibcite{lei_video_2021}{{45}{2021}{{Lei and Huang}}{{}}}
\bibcite{long_improving_2021}{{46}{2021}{{Long et~al\mbox  {.}}}{{}}}
\bibcite{souza_online_2021}{{47}{2021}{{Souza et~al\mbox  {.}}}{{}}}
\bibcite{yazdavar_multimodal_2020}{{48}{2020}{{Yazdavar et~al\mbox  {.}}}{{}}}
\bibcite{huang_what_2021}{{49}{2021}{{Huang et~al\mbox  {.}}}{{}}}
\bibcite{dang2021autumn}{{50}{2021}{{Dang et~al\mbox  {.}}}{{}}}
\bibcite{chu_end--end_2020}{{51}{2020}{{Chu and Yu}}{{}}}
\bibcite{maimaitijiang_soybean_2020}{{52}{2020}{{Maimaitijiang et~al\mbox  {.}}}{{}}}
\bibcite{zhao_battle_2021}{{53}{2021}{{Zhao et~al\mbox  {.}}}{{}}}
\bibcite{clevers_remote_2013}{{54}{2013}{{Clevers and Gitelson}}{{}}}
\bibcite{prakash_multi-modal_2021}{{55}{2021}{{Prakash et~al\mbox  {.}}}{{}}}
\bibcite{nagrani_attention_nodate}{{56}{[n.\,d.]}{{Nagrani et~al\mbox  {.}}}{{}}}
\bibcite{vaswani_attention_2017}{{57}{2017}{{Vaswani et~al\mbox  {.}}}{{}}}
\bibcite{dosovitskiy_image_2021}{{58}{2021}{{Dosovitskiy et~al\mbox  {.}}}{{}}}
\bibcite{fritsch1980monotone}{{59}{1980}{{Fritsch and Carlson}}{{}}}
\bibcite{9672157}{{60}{2022a}{{Patil and Kumar}}{{}}}
\bibcite{9864182}{{61}{2022b}{{Patil and Kumar}}{{}}}
\bibcite{8999620}{{62}{2020}{{Li et~al\mbox  {.}}}{{}}}
\newlabel{tocindent-1}{0pt}
\newlabel{tocindent0}{0pt}
\newlabel{tocindent1}{4.65pt}
\newlabel{tocindent2}{11.49998pt}
\newlabel{tocindent3}{20.22pt}
\newlabel{TotPages}{{24}{24}{}{page.24}{}}
\gdef \@abspage@last{24}
